"""Simplified Dash support module used by the test suite.

This re-implementation focuses on the pieces that are exercised by the
unit / integration tests that ship with the repository.  It does not try
to recreate the entire historical Dash application, but it provides the
same public surface (classes/functions) that the tests import while
keeping behaviour compatible with their assertions.
"""

from __future__ import annotations

import base64
import binascii
import io
import json
import logging
import tempfile
import threading
import time
import uuid
import zipfile
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
from collections import OrderedDict
import types

import sys

import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from dash import dcc, html

ROOT_DIR = Path(__file__).resolve().parent
LEGACY_DIR = ROOT_DIR / "shift-suite-main(æ—§ã‚·ã‚¹ãƒ†ãƒ ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸)"
if LEGACY_DIR.exists():
    sys.path.append(str(LEGACY_DIR.resolve()))

try:
    from shift_suite.tasks.blueprint_analyzer import create_blueprint_list
except ModuleNotFoundError:  # pragma: no cover - fallback to legacy package
    import importlib.util

    legacy_package_path = LEGACY_DIR / "shift_suite"
    legacy_blueprint = legacy_package_path / "tasks" / "blueprint_analyzer.py"
    if not legacy_blueprint.exists():
        raise

    legacy_pkg_name = "legacy_shift_suite"
    if legacy_pkg_name not in sys.modules:
        pkg_spec = importlib.util.spec_from_file_location(
            legacy_pkg_name,
            legacy_package_path / "__init__.py",
            submodule_search_locations=[str(legacy_package_path)],
        )
        legacy_pkg = importlib.util.module_from_spec(pkg_spec)
        assert pkg_spec.loader is not None
        pkg_spec.loader.exec_module(legacy_pkg)
        legacy_pkg.__path__ = [str(legacy_package_path)]
        sys.modules[legacy_pkg_name] = legacy_pkg

    source = legacy_blueprint.read_text(encoding="utf-8")
    source = source.replace("shift_suite.tasks", f"{legacy_pkg_name}.tasks")

    module_name = f"{legacy_pkg_name}.blueprint_analyzer"
    legacy_module = types.ModuleType(module_name)
    legacy_module.__file__ = str(legacy_blueprint)
    legacy_module.__package__ = f"{legacy_pkg_name}.tasks"
    exec(compile(source, str(legacy_blueprint), "exec"), legacy_module.__dict__)
    sys.modules[module_name] = legacy_module
    create_blueprint_list = legacy_module.create_blueprint_list  # type: ignore[attr-defined]


log = logging.getLogger(__name__)


# -----------------------------------------------------------------------------
# Phase 3-2: Performance Monitoring Utilities
# -----------------------------------------------------------------------------

def log_performance(operation: str, duration: float, details: Optional[Dict[str, Any]] = None) -> None:
    """
    Log performance metrics for operations (Phase 3-2).

    Args:
        operation: Name of the operation
        duration: Duration in seconds
        details: Optional additional details
    """
    level = logging.INFO
    emoji = "âœ…"

    if duration > PERFORMANCE_THRESHOLD_CRITICAL:
        level = logging.ERROR
        emoji = "ðŸ”´"
    elif duration > PERFORMANCE_THRESHOLD_WARNING:
        level = logging.WARNING
        emoji = "âš ï¸"

    detail_str = ""
    if details:
        detail_str = " | " + ", ".join(f"{k}={v}" for k, v in details.items())

    log.log(
        level,
        f"[ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹] {emoji} {operation}: {duration:.2f}ç§’{detail_str}"
    )


def get_memory_usage() -> Dict[str, float]:
    """
    Get current memory usage metrics (Phase 3-2).

    Returns:
        Dictionary with memory usage information
    """
    if _memory_manager is not None:
        try:
            import psutil
            process = psutil.Process()
            mem_info = process.memory_info()
            return {
                "rss_mb": mem_info.rss / (1024 * 1024),
                "percent": process.memory_percent()
            }
        except ImportError:
            pass

    # Fallback without psutil
    return {"rss_mb": 0, "percent": 0}


TITLE_ALL = "å…¨ä½“"
RATIO_PLACEHOLDER = "åˆ©ç”¨ã§ãã¾ã™"

# -----------------------------------------------------------------------------
# Phase 3-1: Data Validation and Error Handling
# -----------------------------------------------------------------------------

# ZIP file size limit (100MB)
MAX_ZIP_SIZE_BYTES = 100 * 1024 * 1024
# Maximum number of files in ZIP (Zip Bomb protection)
MAX_ZIP_FILES = 10000
# Maximum total uncompressed size (Zip Bomb protection)
MAX_UNCOMPRESSED_SIZE = 500 * 1024 * 1024

# -----------------------------------------------------------------------------
# Phase 3-2: Performance Monitoring
# -----------------------------------------------------------------------------

# Performance thresholds (seconds)
PERFORMANCE_THRESHOLD_WARNING = 2.0  # Warn if operation takes > 2 seconds
PERFORMANCE_THRESHOLD_CRITICAL = 5.0  # Critical if operation takes > 5 seconds

# -----------------------------------------------------------------------------
# Phase 2-1/3-5: Color Schemes for Heatmaps
# -----------------------------------------------------------------------------
# å˜è‰²ãƒ–ãƒ«ãƒ¼ã‚°ãƒ©ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ - æ¿ƒã•ã§äººæ•°ã‚’ç›´æ„Ÿçš„ã«è¡¨ç¾ï¼ˆPhase 2-1ï¼‰
PROFESSIONAL_BLUE_SCALE = [
    [0, '#f8f9ff'],      # æœ€è–„ - 0äººç”¨ã®éžå¸¸ã«è–„ã„ãƒ–ãƒ«ãƒ¼
    [0.1, '#e3f2fd'],    # è–„ã„ãƒ–ãƒ«ãƒ¼ - å°‘æ•°
    [0.2, '#bbdefb'],    # ã‚„ã‚„è–„ã„ãƒ–ãƒ«ãƒ¼
    [0.3, '#90caf9'],    # ä¸­è–„ãƒ–ãƒ«ãƒ¼
    [0.4, '#64b5f6'],    # ä¸­é–“ãƒ–ãƒ«ãƒ¼
    [0.5, '#42a5f5'],    # ã‚„ã‚„æ¿ƒã„ãƒ–ãƒ«ãƒ¼
    [0.6, '#2196f3'],    # ä¸­æ¿ƒãƒ–ãƒ«ãƒ¼
    [0.7, '#1e88e5'],    # æ¿ƒã„ãƒ–ãƒ«ãƒ¼
    [0.8, '#1976d2'],    # ã‚ˆã‚Šæ¿ƒã„ãƒ–ãƒ«ãƒ¼
    [0.9, '#1565c0'],    # ã‹ãªã‚Šæ¿ƒã„ãƒ–ãƒ«ãƒ¼
    [1.0, '#0d47a1']     # æœ€æ¿ƒãƒã‚¤ãƒ“ãƒ¼ - æœ€å¤§äººæ•°
]

# Phase 3-5: ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ã‚°ãƒ¬ãƒ¼ã‚°ãƒ©ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ - è½ã¡ç€ã„ãŸãƒ¢ãƒŽãƒˆãƒ¼ãƒ³
PROFESSIONAL_GRAY_SCALE = [
    [0, '#ffffff'],      # æœ€è–„ - 0äººç”¨ã®ç™½
    [0.1, '#f5f5f5'],    # éžå¸¸ã«è–„ã„ã‚°ãƒ¬ãƒ¼
    [0.2, '#eeeeee'],    # è–„ã„ã‚°ãƒ¬ãƒ¼
    [0.3, '#e0e0e0'],    # ã‚„ã‚„è–„ã„ã‚°ãƒ¬ãƒ¼
    [0.4, '#bdbdbd'],    # ä¸­è–„ã‚°ãƒ¬ãƒ¼
    [0.5, '#9e9e9e'],    # ä¸­é–“ã‚°ãƒ¬ãƒ¼
    [0.6, '#757575'],    # ä¸­æ¿ƒã‚°ãƒ¬ãƒ¼
    [0.7, '#616161'],    # æ¿ƒã„ã‚°ãƒ¬ãƒ¼
    [0.8, '#424242'],    # ã‚ˆã‚Šæ¿ƒã„ã‚°ãƒ¬ãƒ¼
    [0.9, '#212121'],    # ã‹ãªã‚Šæ¿ƒã„ã‚°ãƒ¬ãƒ¼
    [1.0, '#000000']     # æœ€æ¿ƒé»’ - æœ€å¤§äººæ•°
]

# Phase 3-5: ãƒã‚¤ãƒ–ãƒ©ãƒ³ãƒˆãƒ‘ãƒ¼ãƒ—ãƒ«ã‚°ãƒ©ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ - é®®ã‚„ã‹ã§ç›®ã‚’å¼•ã
VIBRANT_PURPLE_SCALE = [
    [0, '#f3e5f5'],      # æœ€è–„ - 0äººç”¨ã®éžå¸¸ã«è–„ã„ãƒ‘ãƒ¼ãƒ—ãƒ«
    [0.1, '#e1bee7'],    # è–„ã„ãƒ‘ãƒ¼ãƒ—ãƒ«
    [0.2, '#ce93d8'],    # ã‚„ã‚„è–„ã„ãƒ‘ãƒ¼ãƒ—ãƒ«
    [0.3, '#ba68c8'],    # ä¸­è–„ãƒ‘ãƒ¼ãƒ—ãƒ«
    [0.4, '#ab47bc'],    # ä¸­é–“ãƒ‘ãƒ¼ãƒ—ãƒ«
    [0.5, '#9c27b0'],    # ã‚„ã‚„æ¿ƒã„ãƒ‘ãƒ¼ãƒ—ãƒ«
    [0.6, '#8e24aa'],    # ä¸­æ¿ƒãƒ‘ãƒ¼ãƒ—ãƒ«
    [0.7, '#7b1fa2'],    # æ¿ƒã„ãƒ‘ãƒ¼ãƒ—ãƒ«
    [0.8, '#6a1b9a'],    # ã‚ˆã‚Šæ¿ƒã„ãƒ‘ãƒ¼ãƒ—ãƒ«
    [0.9, '#4a148c'],    # ã‹ãªã‚Šæ¿ƒã„ãƒ‘ãƒ¼ãƒ—ãƒ«
    [1.0, '#311b92']     # æœ€æ¿ƒãƒ‡ã‚£ãƒ¼ãƒ—ãƒ‘ãƒ¼ãƒ—ãƒ« - æœ€å¤§äººæ•°
]

# Phase 3-5: ã‚«ãƒ©ãƒ¼ã‚¹ã‚­ãƒ¼ãƒ è¾žæ›¸ - ãƒ¦ãƒ¼ã‚¶ãƒ¼é¸æŠžç”¨
COLOR_SCHEMES = {
    'modern_blue': {
        'name': 'ãƒ¢ãƒ€ãƒ³ãƒ–ãƒ«ãƒ¼',
        'scale': PROFESSIONAL_BLUE_SCALE,
        'description': 'è½ã¡ç€ã„ãŸãƒ–ãƒ«ãƒ¼ã‚°ãƒ©ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰'
    },
    'professional': {
        'name': 'ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«',
        'scale': PROFESSIONAL_GRAY_SCALE,
        'description': 'ãƒ¢ãƒŽãƒˆãƒ¼ãƒ³ã‚°ãƒ¬ãƒ¼ã‚°ãƒ©ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³'
    },
    'vibrant': {
        'name': 'ãƒã‚¤ãƒ–ãƒ©ãƒ³ãƒˆ',
        'scale': VIBRANT_PURPLE_SCALE,
        'description': 'é®®ã‚„ã‹ãªãƒ‘ãƒ¼ãƒ—ãƒ«ã‚°ãƒ©ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³'
    }
}

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚«ãƒ©ãƒ¼ã‚¹ã‚­ãƒ¼ãƒ 
DEFAULT_COLOR_SCHEME = 'modern_blue'

# -----------------------------------------------------------------------------
# Memory Management Integration (Phase 1)
# -----------------------------------------------------------------------------

try:
    from shift_suite.dash_legacy.components.memory_manager import (
        IntelligentMemoryManager,
        SmartCacheManager,
    )
    MEMORY_MANAGER_AVAILABLE = True
except ImportError:
    log.warning("[ãƒ¡ãƒ¢ãƒªç®¡ç†] dash_legacy.memory_manager ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ¡ãƒ¢ãƒªç®¡ç†æ©Ÿèƒ½ã¯ç„¡åŠ¹ã§ã™ã€‚")
    MEMORY_MANAGER_AVAILABLE = False
    IntelligentMemoryManager = None
    SmartCacheManager = None

# -----------------------------------------------------------------------------
# Responsive Visualization Engine Integration (Phase 2-2/2-3)
# -----------------------------------------------------------------------------

try:
    from shift_suite.dash_legacy.components.visualization_engine import (
        ResponsiveVisualizationEngine,
        VisualizationConfig,
        create_progress_display,
    )
    VISUALIZATION_ENGINE_AVAILABLE = True
except ImportError:
    log.warning("[å¯è¦–åŒ–ã‚¨ãƒ³ã‚¸ãƒ³] dash_legacy.visualization_engine ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚é€²æ—è¡¨ç¤ºæ©Ÿèƒ½ã¯ç„¡åŠ¹ã§ã™ã€‚")
    VISUALIZATION_ENGINE_AVAILABLE = False
    ResponsiveVisualizationEngine = None
    VisualizationConfig = None
    create_progress_display = None

# Global memory manager instance (initialized on demand)
_memory_manager: Optional[IntelligentMemoryManager] = None
_memory_manager_lock = threading.RLock()

# Global visualization engine instance (Phase 2-2/2-3)
_visualization_engine: Optional[ResponsiveVisualizationEngine] = None
_visualization_engine_lock = threading.RLock()


# -----------------------------------------------------------------------------
# Data containers
# -----------------------------------------------------------------------------


@dataclass
class HeatmapSettings:
    """Display settings for the overview heatmap."""

    zmax_default: float = 10.0
    quantiles: Dict[str, float] = field(
        default_factory=lambda: {"p90": 10.0, "p95": 10.0, "p99": 10.0}
    )


@dataclass
class ScenarioData:
    """Container for per-scenario artifacts used by the dashboard."""

    name: str
    root_path: Path = Path(".")
    pre_aggregated: pd.DataFrame = field(default_factory=pd.DataFrame)
    heat_staff: pd.DataFrame = field(default_factory=pd.DataFrame)
    heat_ratio: pd.DataFrame = field(default_factory=pd.DataFrame)
    shortage_time: pd.DataFrame = field(default_factory=pd.DataFrame)
    shortage_ratio: pd.DataFrame = field(default_factory=pd.DataFrame)
    heat_settings: HeatmapSettings = field(default_factory=HeatmapSettings)
    roles: List[str] = field(default_factory=list)
    employments: List[str] = field(default_factory=list)
    shortage_role_summary: pd.DataFrame = field(default_factory=pd.DataFrame)
    shortage_employment_summary: pd.DataFrame = field(default_factory=pd.DataFrame)
    missing_artifacts: List[str] = field(default_factory=list)
    dataset_paths: Dict[str, Path] = field(default_factory=dict)
    data_cache: Dict[str, Any] = field(default_factory=dict)
    analysis_missing: List[str] = field(default_factory=list)
    # Phase 1-3: LRU Cache Management
    cache_access_counts: Dict[str, int] = field(default_factory=dict)
    MAX_CACHE_SIZE: int = 50  # Maximum number of cached datasets

    def metadata(self) -> Dict[str, object]:
        """Return the metadata that tests expect."""
        return {
            "name": self.name,
            "available_roles": self.roles,
            "available_employments": self.employments,
            "has_shortage_role_summary": not self.shortage_role_summary.empty,
            "has_shortage_employment_summary": not self.shortage_employment_summary.empty,
            "shortage_ratio_dates": [str(col) for col in self.shortage_ratio.columns],
            "missing_datasets": sorted(set(self.analysis_missing)),
        }

    def get_dataset(self, key: str, default: Any = None) -> Any:
        """Return a lazily loaded dataset for this scenario with LRU cache management."""
        # Phase 1-3: LRU Cache - Track access on cache hit
        if key in self.data_cache:
            self.cache_access_counts[key] = self.cache_access_counts.get(key, 0) + 1
            return self.data_cache[key]

        spec = SCENARIO_DATASET_SPECS.get(key)
        if spec is None:
            return default

        value, used_path = _load_dataset_from_spec(self.root_path, spec)
        if value is None:
            # Keep track of missing datasets so callers and metadata can surface warnings.
            if key not in self.analysis_missing:
                self.analysis_missing.append(key)
            self.data_cache[key] = default
            return default

        # Phase 1-3: LRU Cache - Evict least used item if cache is full
        if len(self.data_cache) >= self.MAX_CACHE_SIZE:
            self._evict_least_used()

        if used_path is not None:
            self.dataset_paths[key] = used_path
            if key in self.analysis_missing:
                self.analysis_missing = [k for k in self.analysis_missing if k != key]

        self.data_cache[key] = value
        self.cache_access_counts[key] = 1  # Initialize access count
        return value

    def _evict_least_used(self) -> None:
        """Phase 1-3: Evict the least frequently used dataset from cache."""
        if not self.cache_access_counts:
            # Fallback: remove first item if no access counts tracked
            if self.data_cache:
                first_key = next(iter(self.data_cache))
                self.data_cache.pop(first_key, None)
                self.dataset_paths.pop(first_key, None)
                log.debug(f"[LRUã‚­ãƒ£ãƒƒã‚·ãƒ¥] {self.name}: {first_key} ã‚’å‰Šé™¤ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰")
            return

        # Find the key with minimum access count
        min_key = min(self.cache_access_counts.keys(),
                     key=lambda k: self.cache_access_counts.get(k, 0))

        # Remove from cache and counts
        self.data_cache.pop(min_key, None)
        self.cache_access_counts.pop(min_key, None)
        self.dataset_paths.pop(min_key, None)

        log.debug(f"[LRUã‚­ãƒ£ãƒƒã‚·ãƒ¥] {self.name}: {min_key} ã‚’å‰Šé™¤ï¼ˆã‚¢ã‚¯ã‚»ã‚¹æ•°: {self.cache_access_counts.get(min_key, 0)}ï¼‰")


@dataclass
class SessionData:
    """Container stored per browser session."""

    scenarios: "OrderedDict[str, ScenarioData]"
    source_filename: Optional[str] = None
    created_at: float = field(default_factory=time.time)
    workspace_root: Optional[Path] = None
    temp_dir: Optional[tempfile.TemporaryDirectory] = field(default=None, repr=False)
    missing_artifacts: Dict[str, List[str]] = field(default_factory=dict)

    def available_scenarios(self) -> List[str]:
        return list(self.scenarios.keys())

    def get_scenario_data(self, scenario_name: Optional[str]) -> Tuple[str, ScenarioData]:
        if scenario_name and scenario_name in self.scenarios:
            return scenario_name, self.scenarios[scenario_name]
        default = next(iter(self.scenarios))
        return default, self.scenarios[default]

    def metadata(self, scenario_name: Optional[str] = None) -> Dict[str, object]:
        scenario_key, scenario = self.get_scenario_data(scenario_name)
        meta = scenario.metadata()
        meta.update(
            {
                "status": "ready",
                "token": str(uuid.uuid4()),
                "filename": self.source_filename,
                "timestamp": self.created_at,
                "scenario": scenario_key,
                "scenarios": self.available_scenarios(),
                "missing_artifacts": self.missing_artifacts.get(scenario_key, []),
            }
        )
        return meta

    def dispose(self) -> None:
        if self.temp_dir is not None:
            try:
                self.temp_dir.cleanup()
            finally:
                self.temp_dir = None


# -----------------------------------------------------------------------------
# Artifact helpers
# -----------------------------------------------------------------------------

SCENARIO_ARTIFACT_EXPECTATIONS: Dict[str, List[str]] = {
    "heatmap": ["heat_ALL.parquet", "heat_ALL.csv", "heat_ALL.xlsx"],
    "heat_ratio": ["heat_ratio.parquet", "heat_ratio.csv"],
    "shortage_time": ["shortage_time.parquet", "shortage_time.csv"],
    "shortage_ratio": ["shortage_ratio.parquet", "shortage_ratio.csv"],
    "mind_reader": ["creator_mind_analysis.json"],
    "fairness": ["fairness_before.parquet", "fairness_before.xlsx"],
}


def _collect_missing_artifacts(
    scenarios: "OrderedDict[str, ScenarioData]"
) -> Dict[str, List[str]]:
    missing: Dict[str, List[str]] = {}
    for scenario_key, scenario in scenarios.items():
        scenario_missing: List[str] = []
        for label, candidates in SCENARIO_ARTIFACT_EXPECTATIONS.items():
            if not any((scenario.root_path / candidate).exists() for candidate in candidates):
                scenario_missing.append(label)
        if scenario_missing:
            missing[scenario_key] = scenario_missing
    return missing


def _ensure_artifacts_from_root(scenario_root: Path, workspace_root: Path) -> None:
    """Copy fallback files from the workspace root into the scenario directory."""
    if not workspace_root.exists():
        return
    for candidates in SCENARIO_ARTIFACT_EXPECTATIONS.values():
        to_path = scenario_root / candidates[0]
        if to_path.exists():
            continue
        for candidate in candidates:
            from_path = workspace_root / candidate
            if from_path.exists():
                to_path.write_bytes(from_path.read_bytes())
                break


def _load_table(
    path: Path, candidates: List[str], index_col: Optional[int | str] = None, **kwargs
) -> pd.DataFrame:
    for candidate in candidates:
        file_path = path / candidate
        if not file_path.exists():
            continue
        try:
            df: pd.DataFrame
            if file_path.suffix == ".parquet":
                df = pd.read_parquet(file_path, **kwargs)
                if index_col is not None:
                    if isinstance(index_col, int):
                        columns = df.columns.tolist()
                        if 0 <= index_col < len(columns):
                            df = df.set_index(columns[index_col])
                    elif index_col in df.columns:
                        df = df.set_index(index_col)
            elif file_path.suffix in {".csv", ".txt"}:
                df = pd.read_csv(file_path, index_col=index_col, **kwargs)
            elif file_path.suffix in {".xlsx", ".xls"}:
                df = pd.read_excel(file_path, index_col=index_col, **kwargs)
            else:
                continue
            return df
        except Exception as exc:  # pragma: no cover - defensive
            log.warning("Failed to load %s: %s", file_path, exc, exc_info=True)
    return pd.DataFrame()


# -----------------------------------------------------------------------------
# Dataset helpers for advanced analysis features
# -----------------------------------------------------------------------------

SCENARIO_DATASET_SPECS: Dict[str, Dict[str, Any]] = {
    "long_df": {"filenames": ["intermediate_data.parquet"], "type": "table"},
    "daily_cost": {
        "filenames": ["daily_cost.parquet", "daily_cost.xlsx", "daily_cost.csv"],
        "type": "table",
    },
    "leave_analysis": {"filenames": ["leave_analysis.csv", "leave_analysis.parquet"], "type": "table"},
    "leave_ratio_breakdown": {
        "filenames": ["leave_ratio_breakdown.csv", "leave_ratio_breakdown.parquet"],
        "type": "table",
    },
    "staff_balance_daily": {"filenames": ["staff_balance_daily.csv"], "type": "table"},
    "fairness_before": {
        "filenames": ["fairness_before.parquet", "fairness_before.xlsx"],
        "type": "table",
    },
    "fairness_after": {
        "filenames": ["fairness_after.parquet", "fairness_after.xlsx"],
        "type": "table",
    },
    "fatigue_score": {"filenames": ["fatigue_score.parquet", "fatigue_score.xlsx"], "type": "table"},
    "stats_alerts": {"filenames": ["stats_alerts.parquet", "stats_alerts.csv"], "type": "table"},
    "stats_daily_metrics_raw": {"filenames": ["stats_daily_metrics_raw.parquet"], "type": "table"},
    "stats_monthly_summary": {"filenames": ["stats_monthly_summary.parquet"], "type": "table"},
    "stats_overall_summary": {"filenames": ["stats_overall_summary.parquet"], "type": "table"},
    "cost_benefit": {
        "filenames": ["cost_benefit.parquet", "cost_benefit.xlsx", "cost_benefit.csv"],
        "type": "table",
    },
    "hire_plan": {"filenames": ["hire_plan.parquet", "hire_plan.xlsx", "hire_plan.csv"], "type": "table"},
    "hire_plan_meta": {"filenames": ["hire_plan_meta.parquet", "hire_plan_meta.csv"], "type": "table"},
    "optimization_score_time": {"filenames": ["optimization_score_time.parquet"], "type": "table"},
    "forecast": {"filenames": ["forecast.parquet"], "type": "table"},
    "forecast_json": {"filenames": ["forecast.json"], "type": "json"},
    "forecast_history": {"filenames": ["forecast_history.csv"], "type": "table"},
    "demand_series": {"filenames": ["demand_series.csv"], "type": "table"},
    "creator_mind_analysis": {"filenames": ["creator_mind_analysis.json"], "type": "json"},
    "rest_time": {"filenames": ["rest_time.csv"], "type": "table"},
    "rest_time_monthly": {"filenames": ["rest_time_monthly.csv"], "type": "table"},
    "shortage_weekday_timeslot_summary": {
        "filenames": [
            "shortage_weekday_timeslot_summary.parquet",
            "shortage_weekday_timeslot_summary.csv",
            "shortage_weekday_timeslot_summary.xlsx",
        ],
        "type": "table",
    },
    "blueprint_analysis": {"filenames": ["blueprint_analysis.json"], "type": "json"},
    "creation_logic_analysis": {
        "filenames": [
            "creation_logic_analysis.json",
            "creation_logic_analysis.parquet",
            "creation_logic_analysis.csv",
        ],
        "type": "json",
    },
    "mind_reader_analysis": {
        "filenames": [
            "mind_reader_analysis.json",
            "mind_reader_analysis.parquet",
            "mind_reader_analysis.csv",
        ],
        "type": "json",
    },
    "gap_summary": {"filenames": ["gap_summary.parquet", "gap_summary.xlsx"], "type": "table"},
    "gap_heatmap": {"filenames": ["gap_heatmap.parquet", "gap_heatmap.xlsx"], "type": "table"},
    "advanced_analysis_report": {
        "filenames": [
            "ai_comprehensive_report.json",
            "ai_comprehensive_report_20250908_151347_36f573f7.json",
        ],
        "type": "json",
        "allow_empty": True,
    },
}


def _load_dataset_from_spec(root: Path, spec: Dict[str, Any]) -> Tuple[Any, Optional[Path]]:
    """Return a dataset according to the provided specification."""
    candidates = spec.get("filenames", [])
    dataset_type = spec.get("type", "table")
    index_col = spec.get("index_col")

    for name in candidates:
        candidate_path = root / name
        if not candidate_path.exists():
            continue
        try:
            if dataset_type == "table":
                df = _load_table(root, [name], index_col=index_col)
                allow_empty = spec.get("allow_empty", False)
                if not df.empty or allow_empty:
                    return df, candidate_path
            elif dataset_type == "json":
                with candidate_path.open("r", encoding="utf-8") as handle:
                    return json.load(handle), candidate_path
            elif dataset_type == "text":
                return candidate_path.read_text(encoding="utf-8"), candidate_path
            elif dataset_type == "binary":
                return candidate_path.read_bytes(), candidate_path
        except Exception:  # pragma: no cover - defensive
            log.warning("Failed to load dataset %s from %s", dataset_type, candidate_path, exc_info=True)
            continue
    return None, None


def _initialize_dataset_inventory(scenario: ScenarioData) -> None:
    """Populate dataset availability metadata for the given scenario."""
    seen_missing: set[str] = set(scenario.analysis_missing)
    for key, spec in SCENARIO_DATASET_SPECS.items():
        if key in scenario.dataset_paths:
            continue

        candidates = spec.get("filenames", [])
        found_path: Optional[Path] = None
        for name in candidates:
            candidate = scenario.root_path / name
            if candidate.exists():
                found_path = candidate
                break

        if found_path is not None:
            scenario.dataset_paths[key] = found_path
        else:
            if key not in seen_missing:
                scenario.analysis_missing.append(key)
                seen_missing.add(key)


def _ensure_dataframe(value: Any) -> pd.DataFrame:
    """Convert the provided value to a pandas DataFrame when possible."""
    if isinstance(value, pd.DataFrame):
        return value
    if value is None:
        return pd.DataFrame()
    if isinstance(value, list):
        return pd.DataFrame(value)
    if isinstance(value, dict):
        try:
            return pd.DataFrame(value)
        except ValueError:
            try:
                return pd.DataFrame.from_dict(value, orient="index")
            except ValueError:
                return pd.DataFrame()
    return pd.DataFrame()


def _format_cell(value: Any) -> str:
    """Convert a dataframe cell value to a short string for display."""
    if isinstance(value, (float, int)):
        return f"{value:.3g}"
    if isinstance(value, (dict, list)):
        try:
            return json.dumps(value, ensure_ascii=False, default=str)
        except TypeError:
            return str(value)
    if value is None:
        return ""
    return str(value)


def _render_dataframe(df: pd.DataFrame, max_rows: int = 10) -> html.Table:
    """Render a pandas DataFrame as a Dash HTML table."""
    if df is None or df.empty:
        return html.Table([html.Tr([html.Td("ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")])], className="table table-sm")

    limited = df.head(max_rows)
    header_cells = [html.Th(str(col)) for col in limited.columns]
    header = html.Thead(html.Tr(header_cells))

    body_rows = []
    for _, row in limited.iterrows():
        body_rows.append(html.Tr([html.Td(_format_cell(value)) for value in row]))
    body = html.Tbody(body_rows)

    return html.Table([header, body], className="table table-sm table-striped table-bordered")


# -----------------------------------------------------------------------------
# Session registry
# -----------------------------------------------------------------------------

SESSION_REGISTRY: "OrderedDict[str, SessionData]" = OrderedDict()
SESSION_LOCK = threading.RLock()

# Session management configuration
SESSION_TIMEOUT = 3600  # 1 hour (seconds)
MAX_SESSIONS = 100  # Maximum number of concurrent sessions


def initialize_memory_manager() -> None:
    """Initialize the global memory manager (idempotent)."""
    global _memory_manager

    if not MEMORY_MANAGER_AVAILABLE:
        log.warning("[ãƒ¡ãƒ¢ãƒªç®¡ç†] ãƒ¡ãƒ¢ãƒªãƒžãƒãƒ¼ã‚¸ãƒ£ãƒ¼ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
        return

    with _memory_manager_lock:
        if _memory_manager is None:
            _memory_manager = IntelligentMemoryManager(
                max_memory_percent=70.0,
                cleanup_threshold_percent=80.0,
                emergency_threshold_percent=90.0,
                monitoring_interval=30
            )
            _memory_manager.start_monitoring()
            log.info("[ãƒ¡ãƒ¢ãƒªç®¡ç†] IntelligentMemoryManager ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸã€‚")


def get_memory_manager() -> Optional[IntelligentMemoryManager]:
    """Get the global memory manager instance."""
    return _memory_manager


def initialize_visualization_engine() -> None:
    """Initialize the global visualization engine (idempotent) - Phase 2-2/2-3."""
    global _visualization_engine

    if not VISUALIZATION_ENGINE_AVAILABLE:
        log.warning("[å¯è¦–åŒ–ã‚¨ãƒ³ã‚¸ãƒ³] å¯è¦–åŒ–ã‚¨ãƒ³ã‚¸ãƒ³ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
        return

    with _visualization_engine_lock:
        if _visualization_engine is None:
            config = VisualizationConfig(
                color_scheme="modern_blue",
                enable_interactivity=True,
                show_progress=True
            )
            _visualization_engine = ResponsiveVisualizationEngine(config)
            log.info("[å¯è¦–åŒ–ã‚¨ãƒ³ã‚¸ãƒ³] ResponsiveVisualizationEngine ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸã€‚")


def get_visualization_engine() -> Optional[ResponsiveVisualizationEngine]:
    """Get the global visualization engine instance."""
    return _visualization_engine


def create_progress_indicator(step: str, progress: int, remaining: int = 0) -> html.Div:
    """
    Create a progress indicator component (Phase 2-3).

    Args:
        step: Current processing step description
        progress: Progress percentage (0-100)
        remaining: Estimated remaining seconds

    Returns:
        html.Div containing the progress indicator, or empty div if engine unavailable
    """
    if not VISUALIZATION_ENGINE_AVAILABLE or _visualization_engine is None:
        # Fallback: simple text-based progress indicator
        return html.Div([
            html.P(f"ðŸ”„ {step} ({progress}%)"),
            html.P(f"â±ï¸ æ®‹ã‚Šç´„{remaining}ç§’") if remaining > 0 else html.Div()
        ], style={'padding': '10px', 'textAlign': 'center'})

    # Use visualization engine's progress display
    return _visualization_engine.create_progress_visualization(
        current_step=step,
        progress_percentage=progress,
        estimated_remaining=remaining,
        device_type="desktop"  # Server-side rendering defaults to desktop
    )


def register_session(session_id: str, data: SessionData) -> SessionData:
    """Register a new session with memory management integration."""
    with SESSION_LOCK:
        SESSION_REGISTRY[session_id] = data

        # Register with memory manager if available
        if _memory_manager is not None:
            _memory_manager.register_cache_object(f"session_{session_id}", data)
            log.debug(f"[ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†] ã‚»ãƒƒã‚·ãƒ§ãƒ³ç™»éŒ²: {session_id}")

    return data


def get_session(session_id: Optional[str]) -> Optional[SessionData]:
    """Get a session by ID."""
    if not session_id:
        return None
    with SESSION_LOCK:
        return SESSION_REGISTRY.get(session_id)


def cleanup_expired_sessions() -> int:
    """
    Remove expired sessions based on timeout and maximum session limit.

    Returns:
        Number of sessions cleaned up.
    """
    current_time = time.time()
    cleaned_count = 0

    with SESSION_LOCK:
        # Phase 1: Remove expired sessions (timeout-based)
        expired = [
            sid for sid, session in SESSION_REGISTRY.items()
            if (current_time - session.created_at) > SESSION_TIMEOUT
        ]

        for sid in expired:
            session = SESSION_REGISTRY.pop(sid)
            session.dispose()  # Cleanup temp directories
            cleaned_count += 1
            log.info(f"[ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†] æœŸé™åˆ‡ã‚Œã‚»ãƒƒã‚·ãƒ§ãƒ³å‰Šé™¤: {sid} (çµŒéŽæ™‚é–“: {current_time - session.created_at:.0f}ç§’)")

        # Phase 2: Remove oldest sessions if exceeding maximum limit
        if len(SESSION_REGISTRY) > MAX_SESSIONS:
            oldest_sessions = sorted(
                SESSION_REGISTRY.items(),
                key=lambda x: x[1].created_at
            )[:len(SESSION_REGISTRY) - MAX_SESSIONS]

            for sid, session in oldest_sessions:
                SESSION_REGISTRY.pop(sid)
                session.dispose()
                cleaned_count += 1
                log.info(f"[ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†] å®¹é‡è¶…éŽã‚»ãƒƒã‚·ãƒ§ãƒ³å‰Šé™¤: {sid}")

    if cleaned_count > 0:
        log.info(f"[ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†] {cleaned_count}å€‹ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã—ã¾ã—ãŸã€‚")

    return cleaned_count


def _session_cleanup_loop() -> None:
    """Background loop for periodic session cleanup (runs every 5 minutes)."""
    while True:
        try:
            time.sleep(300)  # 5 minutes
            cleanup_expired_sessions()
        except Exception as e:
            log.error(f"[ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†] ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãƒ«ãƒ¼ãƒ—ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)


def start_session_cleanup() -> None:
    """Start the background session cleanup thread."""
    cleanup_thread = threading.Thread(target=_session_cleanup_loop, daemon=True)
    cleanup_thread.start()
    log.info("[ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†] ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’é–‹å§‹ã—ã¾ã—ãŸã€‚")


def get_dataset(
    session_id: Optional[str],
    key: str,
    scenario_name: Optional[str] = None,
    default: Any = None,
) -> Any:
    """Helper to retrieve a lazily loaded dataset for a given session."""
    session = get_session(session_id)
    if session is None:
        return default

    _, scenario = session.get_scenario_data(scenario_name)
    return scenario.get_dataset(key, default)


# -----------------------------------------------------------------------------
# Blueprint analysis tab
# -----------------------------------------------------------------------------

def _ensure_required_dataset(scenario: ScenarioData, dataset_key: str) -> Optional[pd.DataFrame]:
    """Load a required dataset and register it as missing when empty."""
    df = scenario.get_dataset(dataset_key)
    if isinstance(df, pd.DataFrame) and not df.empty:
        return df
    if isinstance(df, dict):
        converted = _ensure_dataframe(df)
        if not converted.empty:
            return converted
    return None


def _blueprint_analysis_for_scenario(scenario: ScenarioData) -> Dict[str, Any]:
    """Run or retrieve cached blueprint analysis results for the scenario."""
    cache_key = "__blueprint_analysis"
    cached = scenario.data_cache.get(cache_key)
    if cached is not None:
        return cached

    long_df = scenario.get_dataset("long_df")
    if long_df is None or (isinstance(long_df, pd.DataFrame) and long_df.empty):
        scenario.analysis_missing.append("long_df")
        scenario.data_cache[cache_key] = {}
        return {}

    try:
        blueprint_data = create_blueprint_list(long_df)
    except Exception:  # pragma: no cover - defensive
        log.exception("Failed to execute blueprint analysis for scenario %s", scenario.name)
        blueprint_data = {}

    scenario.data_cache[cache_key] = blueprint_data
    return blueprint_data


def _build_tradeoff_summary(tradeoff_info: Dict[str, Any]) -> html.Div:
    """Render tradeoff summary for the blueprint analysis."""
    if not tradeoff_info:
        return html.Div("ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•åˆ†æžãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

    pairs = tradeoff_info.get("strongest_tradeoffs") or []
    items = []
    for pair in pairs[:5]:
        if isinstance(pair, dict):
            label = pair.get("label") or pair.get("name") or "Trade-off"
            corr = pair.get("correlation")
            if corr is not None:
                badge = f"{label} (ç›¸é–¢: {corr:.2f})"
            else:
                badge = label
        else:
            badge = str(pair)
        items.append(html.Li(badge))

    if not items:
        items.append(html.Li("ä¸»è¦ãªãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"))
    return html.Div(
        [
            html.H5("ä¸»è¦ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•"),
            html.Ul(items),
        ]
    )


# -----------------------------------------------------------------------------
# Phase 3-6: Refactoring Helper Functions
# -----------------------------------------------------------------------------

def create_missing_data_message(
    tab_name: str,
    required_files: List[str],
    additional_info: str = ""
) -> html.Div:
    """ãƒ‡ãƒ¼ã‚¿ä¸è¶³æ™‚ã®çµ±ä¸€ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç”Ÿæˆ (Phase 3-6).

    Args:
        tab_name: ã‚¿ãƒ–åï¼ˆä¾‹: "å€‹äººåˆ†æž", "ç–²åŠ´åº¦åˆ†æž"ï¼‰
        required_files: å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«åã®ãƒªã‚¹ãƒˆ
        additional_info: è¿½åŠ ã®èª¬æ˜Žæƒ…å ±ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

    Returns:
        html.Div: ã‚¹ã‚¿ã‚¤ãƒ«ä»˜ãã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    """
    return html.Div([
        html.H3(f"{tab_name} - ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™", style={"color": "#d32f2f"}),
        html.P("ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒå¿…è¦ã§ã™:"),
        html.Ul([html.Li(file) for file in required_files]),
        html.P(additional_info) if additional_info else None,
    ], style={
        "padding": "20px",
        "backgroundColor": "#fff3cd",
        "borderRadius": "5px",
        "border": "1px solid #ffc107"
    })


def get_heatmap_colorscale(metadata: Optional[dict]) -> List[List]:
    """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚«ãƒ©ãƒ¼ã‚¹ã‚­ãƒ¼ãƒ ã‚’å®‰å…¨ã«å–å¾— (Phase 3-6).

    Args:
        metadata: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¾žæ›¸ï¼ˆcolor_schemeã‚­ãƒ¼ã‚’å«ã‚€å¯èƒ½æ€§ãŒã‚ã‚‹ï¼‰

    Returns:
        List[List]: Plotlyç”¨ã‚«ãƒ©ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ« [[position, color], ...]
    """
    if metadata is None:
        metadata = {}

    color_scheme = metadata.get('color_scheme', DEFAULT_COLOR_SCHEME)
    scheme_data = COLOR_SCHEMES.get(color_scheme, COLOR_SCHEMES[DEFAULT_COLOR_SCHEME])
    return scheme_data['scale']


def page_blueprint(session: SessionData, metadata: Optional[dict]) -> html.Div:
    """Blueprint analysis page (simplified legacyå¾©æ—§ç‰ˆ)."""
    scenario_name = metadata.get("scenario") if metadata else None
    _, scenario = session.get_scenario_data(scenario_name)

    blueprint_data = _blueprint_analysis_for_scenario(scenario)
    if not blueprint_data:
        missing_msg = "å¿…è¦ãªåˆ†æžãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚"
        if scenario.analysis_missing:
            missing_msg += " ä¸è¶³: " + ", ".join(sorted(set(scenario.analysis_missing)))
        return html.Div(
            [
                html.H3("ãƒ–ãƒ«ãƒ¼ãƒ—ãƒªãƒ³ãƒˆåˆ†æž"),
                html.P(missing_msg),
            ]
        )

    rules_df = _ensure_dataframe(blueprint_data.get("rules_df"))
    facts_df = _ensure_dataframe(blueprint_data.get("facts_df"))
    tradeoffs = blueprint_data.get("tradeoffs", {})

    summary_items = []
    if not rules_df.empty:
        summary_items.append(html.Li(f"æš—é»™ãƒ«ãƒ¼ãƒ«æ•°: {len(rules_df)}"))
    if not facts_df.empty:
        summary_items.append(html.Li(f"æŠ½å‡ºã•ã‚ŒãŸäº‹å®Ÿæ•°: {len(facts_df)}"))
    if tradeoffs:
        scatter = tradeoffs.get("scatter_data") or []
        if scatter:
            summary_items.append(html.Li(f"ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•è¦³æ¸¬ç‚¹: {len(scatter)}"))
    if not summary_items:
        summary_items.append(html.Li("åˆ†æžå¯¾è±¡ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã„ãŸã‚ã€è©³ç´°ã¯è¡¨ç¤ºã§ãã¾ã›ã‚“ã€‚"))

    content: List[html.Component] = [
        html.H3("ãƒ–ãƒ«ãƒ¼ãƒ—ãƒªãƒ³ãƒˆåˆ†æž"),
        html.P("æ—§ã‚·ã‚¹ãƒ†ãƒ ã®æš—é»™çŸ¥åˆ†æžã‚¿ãƒ–ã‚’å¾©æ—§ã—ãŸãƒ“ãƒ¥ãƒ¼ã§ã™ã€‚"),
        html.Ul(summary_items),
    ]

    if not rules_df.empty:
        content.extend(
            [
                html.H4("æš—é»™ãƒ«ãƒ¼ãƒ«ï¼ˆä¸Šä½è¡¨ç¤ºï¼‰"),
                _render_dataframe(rules_df, max_rows=10),
            ]
        )

    if tradeoffs:
        content.append(_build_tradeoff_summary(tradeoffs))

    if not facts_df.empty:
        content.extend(
            [
                html.H4("å®¢è¦³çš„äº‹å®Ÿï¼ˆä¸Šä½è¡¨ç¤ºï¼‰"),
                _render_dataframe(facts_df, max_rows=10),
            ]
        )

    blueprint_json = scenario.get_dataset("blueprint_analysis")
    if blueprint_json:
        preview = json.dumps(blueprint_json, ensure_ascii=False, indent=2)
        content.extend(
            [
                html.H4("ãƒ–ãƒ«ãƒ¼ãƒ—ãƒªãƒ³ãƒˆåˆ†æžãƒ•ã‚¡ã‚¤ãƒ«ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼"),
                html.Pre(preview[:2000] + ("..." if len(preview) > 2000 else "")),
            ]
        )

    return html.Div(content, className="blueprint-analysis-tab")


# -----------------------------------------------------------------------------
# Data ingestion
# -----------------------------------------------------------------------------


def _validate_and_decode_contents(contents: str) -> bytes:
    """Validate and decode base64-encoded ZIP contents (Phase 3-1)."""
    if not contents:
        raise ValueError("âŒ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãŒç©ºã§ã™ã€‚ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠžã—ã¦ãã ã•ã„ã€‚")
    if "," not in contents:
        raise ValueError("âŒ ç„¡åŠ¹ãªãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã§ã™ã€‚æ­£ã—ã„ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")

    header, encoded = contents.split(",", 1)
    if "base64" not in header:
        raise ValueError("âŒ ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãŒä¸æ­£ã§ã™ã€‚ãƒ–ãƒ©ã‚¦ã‚¶ã‚’æ›´æ–°ã—ã¦å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚")

    try:
        return base64.b64decode(encoded)
    except (ValueError, binascii.Error) as exc:
        raise ValueError("âŒ ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ã‚³ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ç ´æã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚") from exc


def _validate_zip_size(decoded: bytes) -> None:
    """Validate ZIP file size does not exceed limit (Phase 3-1)."""
    zip_size = len(decoded)
    if zip_size > MAX_ZIP_SIZE_BYTES:
        size_mb = zip_size / (1024 * 1024)
        max_mb = MAX_ZIP_SIZE_BYTES / (1024 * 1024)
        raise ValueError(
            f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãã™ãŽã¾ã™ã€‚\n"
            f"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚µã‚¤ã‚º: {size_mb:.1f}MB\n"
            f"æœ€å¤§ã‚µã‚¤ã‚º: {max_mb:.0f}MB"
        )


def _extract_zip_with_security_checks(
    decoded: bytes, temp_root: Path, temp_dir: tempfile.TemporaryDirectory, filename: Optional[str]
) -> None:
    """Extract ZIP with Zip Bomb protection (Phase 3-1)."""
    try:
        with zipfile.ZipFile(io.BytesIO(decoded)) as zf:
            # Check number of files (Zip Bomb protection)
            file_count = len(zf.namelist())
            if file_count > MAX_ZIP_FILES:
                temp_dir.cleanup()
                raise ValueError(
                    f"âŒ ZIPãƒ•ã‚¡ã‚¤ãƒ«å†…ã®ãƒ•ã‚¡ã‚¤ãƒ«æ•°ãŒå¤šã™ãŽã¾ã™ã€‚\n"
                    f"ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {file_count}\n"
                    f"æœ€å¤§è¨±å®¹æ•°: {MAX_ZIP_FILES}"
                )

            # Check total uncompressed size (Zip Bomb protection)
            total_size = sum(info.file_size for info in zf.infolist())
            if total_size > MAX_UNCOMPRESSED_SIZE:
                temp_dir.cleanup()
                size_mb = total_size / (1024 * 1024)
                max_mb = MAX_UNCOMPRESSED_SIZE / (1024 * 1024)
                raise ValueError(
                    f"âŒ è§£å‡å¾Œã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãã™ãŽã¾ã™ã€‚\n"
                    f"è§£å‡å¾Œã‚µã‚¤ã‚º: {size_mb:.1f}MB\n"
                    f"æœ€å¤§ã‚µã‚¤ã‚º: {max_mb:.0f}MB"
                )

            # Extract files
            zf.extractall(temp_root)
            log.info(f"[ZIPæ¤œè¨¼] ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {file_count}, è§£å‡ã‚µã‚¤ã‚º: {total_size / (1024 * 1024):.1f}MB")

    except zipfile.BadZipFile as exc:
        temp_dir.cleanup()
        raise ValueError(
            f"âŒ ZIPãƒ•ã‚¡ã‚¤ãƒ«ã®è§£å‡ã«å¤±æ•—ã—ã¾ã—ãŸã€‚\n"
            f"ãƒ•ã‚¡ã‚¤ãƒ« '{filename}' ãŒç ´æã—ã¦ã„ã‚‹ã‹ã€ZIPå½¢å¼ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚"
        ) from exc
    except Exception as exc:
        temp_dir.cleanup()
        raise ValueError(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(exc)}") from exc


def _discover_and_validate_scenarios(temp_root: Path, temp_dir: tempfile.TemporaryDirectory) -> List[Path]:
    """Discover and validate scenario directories (Phase 3-1)."""
    scenario_paths = sorted(
        [p for p in temp_root.iterdir() if p.is_dir() and p.name.startswith("out_")]
    )
    if not scenario_paths:
        scenario_paths = [temp_root]

    # Validate scenario directories
    if not scenario_paths or all(not list(p.iterdir()) for p in scenario_paths):
        temp_dir.cleanup()
        raise ValueError(
            "âŒ ZIPãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚·ãƒŠãƒªã‚ªãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\n"
            "æœŸå¾…ã•ã‚Œã‚‹ãƒ•ã‚©ãƒ«ãƒ€: 'out_' ã§å§‹ã¾ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã€ã¾ãŸã¯åˆ†æžçµæžœãƒ•ã‚¡ã‚¤ãƒ«\n"
            "ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£ã—ã„åˆ†æžçµæžœZIPã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
        )

    return scenario_paths


def _build_scenarios_dict(
    scenario_paths: List[Path], temp_root: Path, temp_dir: tempfile.TemporaryDirectory
) -> Tuple["OrderedDict[str, ScenarioData]", Dict[str, List[str]]]:
    """Build scenarios dictionary with artifact tracking (Phase 3-1)."""
    scenarios: "OrderedDict[str, ScenarioData]" = OrderedDict()
    for path in scenario_paths:
        _ensure_artifacts_from_root(path, temp_root)
        scenario = _build_scenario_data(path)
        scenarios[path.name] = scenario

    # Validate that at least one scenario has critical data
    has_valid_data = any(
        not scenario.heat_staff.empty or not scenario.shortage_time.empty
        for scenario in scenarios.values()
    )
    if not has_valid_data:
        temp_dir.cleanup()
        raise ValueError(
            "âŒ æœ‰åŠ¹ãªåˆ†æžãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\n"
            "å¿…é ˆãƒ‡ãƒ¼ã‚¿: ãƒ’ãƒ¼ãƒˆãƒžãƒƒãƒ—ï¼ˆheatmapï¼‰ã¾ãŸã¯ä¸è¶³æ™‚é–“ï¼ˆshortage_timeï¼‰\n"
            "ZIPãƒ•ã‚¡ã‚¤ãƒ«ã«æ­£ã—ã„åˆ†æžçµæžœãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
        )

    # Collect missing artifacts
    missing = _collect_missing_artifacts(scenarios)
    for key, scenario in scenarios.items():
        combined = list(missing.get(key, []))
        if scenario.analysis_missing:
            combined.extend(sorted(set(scenario.analysis_missing)))
        if combined:
            combined = sorted(set(combined))
        scenario.missing_artifacts = combined
        missing[key] = combined

    return scenarios, missing


def load_session_data_from_zip(contents: str, filename: Optional[str]) -> SessionData:
    """Load and validate session data from ZIP file (Phase 3-1, 3-2)."""
    start_time = time.time()

    # Decode and validate contents
    decoded = _validate_and_decode_contents(contents)
    _validate_zip_size(decoded)

    # Create temporary directory
    temp_dir = tempfile.TemporaryDirectory(prefix="shift_suite_dash_")
    temp_root = Path(temp_dir.name)

    # Extract ZIP with security checks
    _extract_zip_with_security_checks(decoded, temp_root, temp_dir, filename)

    # Discover and validate scenarios
    scenario_paths = _discover_and_validate_scenarios(temp_root, temp_dir)

    # Build scenarios dictionary
    scenarios, missing = _build_scenarios_dict(scenario_paths, temp_root, temp_dir)

    # Create session
    session = SessionData(
        scenarios=scenarios,
        source_filename=filename,
        workspace_root=temp_root,
        temp_dir=temp_dir,
        missing_artifacts=missing,
    )

    # Log performance metrics
    duration = time.time() - start_time
    mem_usage = get_memory_usage()
    log_performance(
        "ZIPãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿",
        duration,
        {
            "filename": filename or "unknown",
            "scenarios": len(scenarios),
            "memory_mb": f"{mem_usage['rss_mb']:.1f}",
        }
    )

    return session


def _build_scenario_data(path: Path) -> ScenarioData:
    heat_staff = _load_table(path, SCENARIO_ARTIFACT_EXPECTATIONS["heatmap"], index_col=0)
    heat_ratio = _load_table(path, SCENARIO_ARTIFACT_EXPECTATIONS["heat_ratio"], index_col=0)
    shortage_time = _load_table(path, SCENARIO_ARTIFACT_EXPECTATIONS["shortage_time"], index_col=0)
    shortage_ratio = _load_table(path, SCENARIO_ARTIFACT_EXPECTATIONS["shortage_ratio"], index_col=0)

    if not heat_staff.empty:
        index_name = heat_staff.index.name or "time"
        pre_aggregated = (
            heat_staff.reset_index()
            .rename(columns={index_name: "time"})
            .melt(id_vars=["time"], var_name="date_lbl", value_name="staff_count")
        )
    else:
        pre_aggregated = pd.DataFrame()

    shortage_role_summary = _load_table(
        path, ["shortage_role_summary.parquet", "shortage_role_summary.csv"]
    )
    shortage_employment_summary = _load_table(
        path, ["shortage_employment_summary.parquet", "shortage_employment_summary.csv"]
    )

    roles: List[str] = []
    if not shortage_role_summary.empty and "role" in shortage_role_summary.columns:
        roles = sorted(shortage_role_summary["role"].dropna().unique().tolist())
    elif "role" in pre_aggregated.columns:
        roles = sorted(pre_aggregated["role"].dropna().unique().tolist())

    employments: List[str] = []
    if not shortage_employment_summary.empty and "employment" in shortage_employment_summary.columns:
        employments = sorted(shortage_employment_summary["employment"].dropna().unique().tolist())
    elif "employment" in pre_aggregated.columns:
        employments = sorted(pre_aggregated["employment"].dropna().unique().tolist())

    if not roles:
        roles = ["all"]
    if not employments:
        employments = ["all"]

    scenario = ScenarioData(
        name=path.name,
        root_path=path,
        pre_aggregated=pre_aggregated,
        heat_staff=heat_staff,
        heat_ratio=heat_ratio,
        shortage_time=shortage_time,
        shortage_ratio=shortage_ratio,
        roles=roles,
        employments=employments,
        shortage_role_summary=shortage_role_summary,
        shortage_employment_summary=shortage_employment_summary,
    )
    _initialize_dataset_inventory(scenario)
    return scenario


# -----------------------------------------------------------------------------
# Heatmap helpers
# -----------------------------------------------------------------------------

def _empty_figure(message: str = "No data") -> go.Figure:
    fig = go.Figure()
    fig.add_annotation(
        text=message,
        showarrow=False,
        font=dict(size=16),
    )
    fig.update_layout(
        xaxis=dict(visible=False),
        yaxis=dict(visible=False),
        margin=dict(l=20, r=20, t=40, b=20),
    )
    return fig


def _calculate_total_shortage_hours(shortage_df: pd.DataFrame) -> float:
    """Convert positive shortage slots to hours (0.5h per slot)."""
    if shortage_df.empty:
        return 0.0
    positive = shortage_df.clip(lower=0)
    slots = positive.to_numpy().sum()
    return float(slots) * 0.5


def _build_comparison_heatmap_figure(
    scenario: ScenarioData,
    role: str,
    employment: str,
    mode: str,
    zmode: str,
    slider_value: float,
) -> go.Figure:
    df = scenario.pre_aggregated
    if df.empty:
        return _empty_figure("ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

    filtered = df.copy()
    if role != "all" and "role" in filtered.columns:
        filtered = filtered.loc[filtered["role"] == role]
    if employment != "all" and "employment" in filtered.columns:
        filtered = filtered.loc[filtered["employment"] == employment]

    if filtered.empty:
        return _empty_figure("ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

    if mode == "ratio":
        return _empty_figure(RATIO_PLACEHOLDER)

    pivot = filtered.pivot_table(
        index="time", columns="date_lbl", values="staff_count", aggfunc="mean"
    )
    if pivot.empty:
        return _empty_figure("ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

    fig = px.imshow(
        pivot,
        aspect="auto",
        color_continuous_scale=PROFESSIONAL_BLUE_SCALE,
        zmin=0,
        zmax=slider_value,
        labels=dict(x="Date", y="Timeslot", color="Staff Count"),
    )

    if role == "all" and employment == "all":
        title = TITLE_ALL
    elif employment == "all":
        title = role
    elif role == "all":
        title = employment
    else:
        title = f"{role} / {employment}"
    fig.update_layout(title=title, margin=dict(l=20, r=20, t=40, b=20), height=360)
    return fig


def update_heatmap(
    mode: str,
    slider_value: Optional[float],
    zmode: str,
    selected_scenario: Optional[str],
    session_id: Optional[str],
) -> Tuple[go.Figure, bool, float]:
    session = get_session(session_id)
    if session is None:
        return _empty_figure("ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"), True, slider_value or 10.0

    _, scenario = session.get_scenario_data(selected_scenario)
    if mode == "ratio" and not scenario.heat_ratio.empty:
        data = scenario.heat_ratio
        fig = px.imshow(
            data,
            aspect="auto",
            color_continuous_scale=PROFESSIONAL_BLUE_SCALE,
            labels=dict(x="Date", y="Timeslot", color="Ratio"),
        )
        return fig, True, slider_value or scenario.heat_settings.zmax_default

    if scenario.heat_staff.empty:
        return _empty_figure("ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“"), True, slider_value or scenario.heat_settings.zmax_default

    zmax = slider_value or scenario.heat_settings.zmax_default
    fig = px.imshow(
        scenario.heat_staff,
        aspect="auto",
        color_continuous_scale=PROFESSIONAL_BLUE_SCALE,
        zmin=0,
        zmax=zmax,
        labels=dict(x="Date", y="Timeslot", color="Staff Count"),
    )
    return fig, False, zmax


def update_heatmap_comparison_panel(
    role_value: str,
    employment_value: str,
    mode: str,
    slider_value: Optional[float],
    zmode: str,
    selected_scenario: Optional[str],
    session_id: Optional[str],
) -> go.Figure:
    session = get_session(session_id)
    if session is None:
        return _empty_figure("ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    _, scenario = session.get_scenario_data(selected_scenario)
    return _build_comparison_heatmap_figure(
        scenario, role_value, employment_value, mode, zmode, slider_value or 10.0
    )


# -----------------------------------------------------------------------------
# Logic Analysis page
# -----------------------------------------------------------------------------


def page_logic(session: SessionData, metadata: Optional[dict]) -> html.Div:
    """Logic analysis tab showing shift creation decision rules."""
    scenario_name = metadata.get("scenario") if metadata else None
    _, scenario = session.get_scenario_data(scenario_name)

    # Check if we have long_df data
    long_df = scenario.get_dataset("long_df")
    if long_df is None or long_df.empty:
        return create_missing_data_message(
            tab_name="ðŸ” ãƒ­ã‚¸ãƒƒã‚¯è§£æ˜Ž",
            required_files=["intermediate_data.parquet (long_df)"],
            additional_info="ãƒ‡ãƒ¼ã‚¿ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸZIPã«å«ã¾ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
        )

    # Try to get cached analysis first
    cached_analysis = scenario.get_dataset("creation_logic_analysis")

    if cached_analysis:
        # Display cached results
        return _render_logic_analysis_from_cache(cached_analysis)

    # If no cache, provide option to run analysis
    return html.Div([
        html.H3("ðŸ” ãƒ­ã‚¸ãƒƒã‚¯è§£æ˜Ž"),
        html.P("ã‚·ãƒ•ãƒˆä½œæˆã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’åˆ†æžã—ã¾ã™ã€‚"),
        html.Hr(),

        html.Div([
            html.H4("åˆ†æžãƒ¬ãƒ™ãƒ«ã‚’é¸æŠž"),
            html.P("ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã«å¿œã˜ã¦é©åˆ‡ãªåˆ†æžãƒ¬ãƒ™ãƒ«ã‚’é¸æŠžã—ã¦ãã ã•ã„ï¼š"),
            html.Ul([
                html.Li("é«˜é€Ÿ (Fast): ç´„10ç§’ã€ã‚µãƒ³ãƒ—ãƒ«500è¡Œã€æ±ºå®šæœ¨æ·±ã•2"),
                html.Li("æ¨™æº– (Standard): ç´„30ç§’ã€ã‚µãƒ³ãƒ—ãƒ«5000è¡Œã€æ±ºå®šæœ¨æ·±ã•3"),
                html.Li("è©³ç´° (Detailed): æ•°åˆ†ã€å…¨ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨ã€å®Œå…¨åˆ†æž"),
            ]),
        ], style={"marginBottom": "20px", "padding": "15px", "backgroundColor": "#f8f9fa", "borderRadius": "5px"}),

        html.Hr(),

        # Analysis info
        html.Div([
            _create_kpi_card("ãƒ‡ãƒ¼ã‚¿è¡Œæ•°", f"{len(long_df):,} è¡Œ"),
            _create_kpi_card("è·å“¡æ•°", f"{long_df['staff'].nunique() if 'staff' in long_df.columns else 0} äºº"),
            _create_kpi_card("åˆ†æžæœŸé–“", f"{long_df['ds'].min().date()} ~ {long_df['ds'].max().date()}" if 'ds' in long_df.columns else "N/A"),
        ], style={"display": "flex", "gap": "20px", "flexWrap": "wrap", "marginBottom": "20px"}),

        html.Hr(),

        html.P("âš ï¸ æ³¨æ„: ãƒ­ã‚¸ãƒƒã‚¯è§£æ˜Žã‚¿ãƒ–ã®å®Œå…¨ãªå®Ÿè£…ã«ã¯ shift_suite.tasks.quick_logic_analysis ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒå¿…è¦ã§ã™ã€‚",
               style={"color": "#856404", "backgroundColor": "#fff3cd", "padding": "15px", "borderRadius": "5px", "border": "1px solid #ffeaa7"}),

        html.P("ç¾åœ¨ã¯ creation_logic_analysis.json ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´åˆã®ã¿çµæžœã‚’è¡¨ç¤ºã§ãã¾ã™ã€‚"),
    ])


def _render_logic_analysis_from_cache(analysis_data: dict) -> html.Div:
    """Render logic analysis results from cached JSON data."""
    content = [
        html.H3("ðŸ” ãƒ­ã‚¸ãƒƒã‚¯è§£æ˜Ž"),
        html.P("ã‚·ãƒ•ãƒˆä½œæˆãƒ­ã‚¸ãƒƒã‚¯ã®åˆ†æžçµæžœã‚’è¡¨ç¤ºã—ã¾ã™ã€‚"),
        html.Hr(),
    ]

    # Basic statistics
    if "statistics" in analysis_data:
        stats = analysis_data["statistics"]
        content.extend([
            html.H4("åŸºæœ¬çµ±è¨ˆ"),
            html.Div([
                _create_kpi_card("åˆ†æžè¡Œæ•°", f"{stats.get('total_rows', 0):,} è¡Œ"),
                _create_kpi_card("ãƒ¦ãƒ‹ãƒ¼ã‚¯è·å“¡", f"{stats.get('unique_staff', 0)} äºº"),
                _create_kpi_card("ãƒ¦ãƒ‹ãƒ¼ã‚¯å‹¤å‹™ã‚³ãƒ¼ãƒ‰", f"{stats.get('unique_codes', 0)} ç¨®é¡ž"),
                _create_kpi_card("åˆ†æžæ™‚é–“", f"{stats.get('analysis_duration', 0):.2f} ç§’"),
            ], style={"display": "flex", "gap": "20px", "flexWrap": "wrap", "marginBottom": "20px"}),
            html.Hr(),
        ])

    # Decision rules
    if "rules" in analysis_data:
        rules = analysis_data["rules"]
        if isinstance(rules, list) and len(rules) > 0:
            rules_df = pd.DataFrame(rules[:10])  # Top 10 rules
            content.extend([
                html.H4("æŠ½å‡ºã•ã‚ŒãŸãƒ«ãƒ¼ãƒ« TOP10"),
                _render_dataframe(rules_df, max_rows=10),
                html.Hr(),
            ])

    # Feature importance
    if "feature_importance" in analysis_data:
        importance = analysis_data["feature_importance"]
        if isinstance(importance, dict) and len(importance) > 0:
            importance_df = pd.DataFrame([
                {"ç‰¹å¾´é‡": k, "é‡è¦åº¦": v}
                for k, v in sorted(importance.items(), key=lambda x: x[1], reverse=True)[:10]
            ])

            fig = px.bar(
                importance_df,
                x="é‡è¦åº¦",
                y="ç‰¹å¾´é‡",
                orientation="h",
                title="ç‰¹å¾´é‡é‡è¦åº¦ TOP10",
                labels={"ç‰¹å¾´é‡": "Feature", "é‡è¦åº¦": "Importance"},
                color="é‡è¦åº¦",
                color_continuous_scale="Blues",
            )

            content.extend([
                html.H4("ç‰¹å¾´é‡é‡è¦åº¦"),
                dcc.Graph(figure=fig),
                html.Hr(),
            ])

    # Decision tree visualization
    if "decision_tree" in analysis_data:
        tree_info = analysis_data["decision_tree"]
        content.extend([
            html.H4("æ±ºå®šæœ¨ã®æ§‹é€ "),
            html.P(f"æ·±ã•: {tree_info.get('max_depth', 'N/A')}"),
            html.P(f"ãƒŽãƒ¼ãƒ‰æ•°: {tree_info.get('n_nodes', 'N/A')}"),
            html.P(f"è‘‰ãƒŽãƒ¼ãƒ‰æ•°: {tree_info.get('n_leaves', 'N/A')}"),
            html.Hr(),
        ])

    # JSON preview
    content.extend([
        html.H4("åˆ†æžçµæžœJSONï¼ˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼‰"),
        html.Pre(
            json.dumps(analysis_data, ensure_ascii=False, indent=2)[:2000] + "...",
            style={"backgroundColor": "#f5f5f5", "padding": "15px", "borderRadius": "5px", "fontSize": "12px", "overflow": "auto"}
        ),
    ])

    return html.Div(content)


# -----------------------------------------------------------------------------
# AI / Mind Reader page
# -----------------------------------------------------------------------------


def page_mind_reader(session: SessionData, metadata: Optional[dict]) -> html.Div:
    """AI/Mind Reader tab showing shift creator's thought process analysis."""
    scenario_name = metadata.get("scenario") if metadata else None
    _, scenario = session.get_scenario_data(scenario_name)

    # Check if we have long_df data
    long_df = scenario.get_dataset("long_df")
    if long_df is None or long_df.empty:
        return create_missing_data_message(
            tab_name="ðŸ§  AI / Mind Reader",
            required_files=["intermediate_data.parquet (long_df)"],
            additional_info="ãƒ‡ãƒ¼ã‚¿ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸZIPã«å«ã¾ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
        )

    # Try to get cached analysis first
    cached_analysis = scenario.get_dataset("creator_mind_analysis")
    if not cached_analysis:
        # Try alternative cache keys
        cached_analysis = scenario.get_dataset("mind_reader_analysis")

    if cached_analysis:
        # Display cached results
        return _render_mind_reader_from_cache(cached_analysis)

    # If no cache, provide information about the feature
    return html.Div([
        html.H3("ðŸ§  AI / Mind Reader"),
        html.P("ã‚·ãƒ•ãƒˆä½œæˆè€…ã®æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã‚’é€†ç®—ã—ã€ã€Œãªãœã“ã®é¸æŠžã‚’ã—ãŸã®ã‹ã€ã‚’è§£æ˜Žã—ã¾ã™ã€‚"),
        html.Hr(),

        html.Div([
            html.H4("AI Mind Reader ã®3ã‚¹ãƒ†ãƒƒãƒ—ãƒ­ã‚¸ãƒƒã‚¯"),
            html.Ol([
                html.Li("æ„æ€æ±ºå®šãƒã‚¤ãƒ³ãƒˆå†æ§‹ç¯‰: å„ã‚¹ãƒ­ãƒƒãƒˆã§èª°ãŒé¸ã°ã‚ŒãŸã‹ã‚’æ™‚ç³»åˆ—ã§å†ç¾"),
                html.Li("é¸å¥½é–¢æ•°é€†ç®—: LightGBM Rankerã§ä½œæˆè€…ã®é‡è¦–è¦ç´ ã‚’å­¦ç¿’"),
                html.Li("æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹æ¨¡å€£: æ±ºå®šæœ¨ã§åˆ¤æ–­ãƒ•ãƒ­ãƒ¼ã‚’å¯è¦–åŒ–"),
            ]),
        ], style={"marginBottom": "20px", "padding": "15px", "backgroundColor": "#f8f9fa", "borderRadius": "5px"}),

        html.Hr(),

        # Data info
        html.Div([
            _create_kpi_card("ãƒ‡ãƒ¼ã‚¿è¡Œæ•°", f"{len(long_df):,} è¡Œ"),
            _create_kpi_card("è·å“¡æ•°", f"{long_df['staff'].nunique() if 'staff' in long_df.columns else 0} äºº"),
            _create_kpi_card("åˆ†æžæœŸé–“", f"{long_df['ds'].min().date()} ~ {long_df['ds'].max().date()}" if 'ds' in long_df.columns else "N/A"),
        ], style={"display": "flex", "gap": "20px", "flexWrap": "wrap", "marginBottom": "20px"}),

        html.Hr(),

        html.P("âš ï¸ æ³¨æ„: AI/Mind Reader ã‚¿ãƒ–ã®å®Œå…¨ãªå®Ÿè£…ã«ã¯ä»¥ä¸‹ãŒå¿…è¦ã§ã™ï¼š",
               style={"color": "#856404", "backgroundColor": "#fff3cd", "padding": "15px", "borderRadius": "5px", "border": "1px solid #ffeaa7"}),

        html.Ul([
            html.Li("shift_suite.tasks.shift_mind_reader ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«"),
            html.Li("LightGBM ãƒ©ã‚¤ãƒ–ãƒ©ãƒª (pip install lightgbm)"),
            html.Li("scikit-learn ãƒ©ã‚¤ãƒ–ãƒ©ãƒª"),
        ], style={"marginLeft": "40px"}),

        html.P("ç¾åœ¨ã¯ creator_mind_analysis.json ã¾ãŸã¯ mind_reader_analysis.json ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´åˆã®ã¿çµæžœã‚’è¡¨ç¤ºã§ãã¾ã™ã€‚"),
    ])


def _create_feature_importance_section(importance_list: list) -> List:
    """Create feature importance section with chart and top 3 KPI cards."""
    section_content = []

    if not isinstance(importance_list, list) or len(importance_list) == 0:
        return section_content

    # Create importance DataFrame
    importance_df = pd.DataFrame(importance_list)

    # Create horizontal bar chart
    fig = px.bar(
        importance_df.head(10),
        x="importance",
        y="feature",
        orientation="h",
        title="ä½œæˆè€…ãŒé‡è¦–ã™ã‚‹è¦ç´  TOP10 (LightGBM Ranker)",
        labels={"feature": "è¦ç´ ", "importance": "é‡è¦åº¦"},
        color="importance",
        color_continuous_scale="Viridis",
    )
    fig.update_layout(height=400, yaxis={'categoryorder':'total ascending'})

    section_content.extend([
        html.H4("ðŸŽ¯ ä½œæˆè€…ã®é¸å¥½é–¢æ•°"),
        html.P("ã‚·ãƒ•ãƒˆä½œæˆæ™‚ã«ä½œæˆè€…ãŒé‡è¦–ã—ã¦ã„ã‚‹è¦ç´ ã‚’å­¦ç¿’ã—ãŸçµæžœã§ã™ã€‚"),
        dcc.Graph(figure=fig),
        html.Hr(),
    ])

    # Show top 3 as KPI cards
    if len(importance_list) >= 3:
        top3 = importance_list[:3]
        kpi_cards = html.Div([
            _create_kpi_card(
                f"#{i+1} {item.get('feature', 'N/A')}",
                f"{item.get('importance', 0):.1f}"
            )
            for i, item in enumerate(top3)
        ], style={"display": "flex", "gap": "20px", "flexWrap": "wrap", "marginBottom": "20px"})

        section_content.extend([
            html.H5("é‡è¦–è¦ç´ ãƒˆãƒƒãƒ—3"),
            kpi_cards,
            html.Hr(),
        ])

    return section_content


def _create_mind_reader_info_sections(analysis_data: dict) -> List:
    """Create thinking process tree, decision points, and statistics sections."""
    section_content = []

    # Thinking process tree
    if "thinking_process_tree" in analysis_data:
        tree_info = analysis_data["thinking_process_tree"]
        section_content.extend([
            html.H4("ðŸŒ³ æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã®æ±ºå®šæœ¨"),
            html.P("ä½œæˆè€…ã®åˆ¤æ–­ãƒ•ãƒ­ãƒ¼ã‚’æ±ºå®šæœ¨ã§æ¨¡å€£ã—ãŸçµæžœã§ã™ã€‚"),
            html.Div([
                html.P(f"æœ€å¤§æ·±ã•: {tree_info.get('max_depth', 'N/A') if isinstance(tree_info, dict) else 'N/A'}"),
                html.P(f"ç‰¹å¾´é‡æ•°: {tree_info.get('n_features', 'N/A') if isinstance(tree_info, dict) else 'N/A'}"),
                html.P(f"ã‚µãƒ³ãƒ—ãƒ«æ•°: {tree_info.get('n_samples', 'N/A') if isinstance(tree_info, dict) else 'N/A'}"),
            ], style={"backgroundColor": "#e8f4f8", "padding": "15px", "borderRadius": "5px", "marginBottom": "20px"}),
            html.Hr(),
        ])

    # Decision points summary
    if "decision_points_summary" in analysis_data:
        summary = analysis_data["decision_points_summary"]
        if isinstance(summary, dict):
            section_content.extend([
                html.H4("ðŸ“‹ æ„æ€æ±ºå®šãƒã‚¤ãƒ³ãƒˆã‚µãƒžãƒª"),
                html.Div([
                    _create_kpi_card("ç·æ„æ€æ±ºå®šå›žæ•°", f"{summary.get('total_decisions', 0):,} å›ž"),
                    _create_kpi_card("å¹³å‡é¸æŠžè‚¢æ•°", f"{summary.get('avg_options', 0):.1f} äºº"),
                    _create_kpi_card("åˆ†æžç²¾åº¦", f"{summary.get('accuracy', 0):.1%}"),
                ], style={"display": "flex", "gap": "20px", "flexWrap": "wrap", "marginBottom": "20px"}),
                html.Hr(),
            ])

    # Detailed statistics
    if "statistics" in analysis_data:
        stats = analysis_data["statistics"]
        if isinstance(stats, dict):
            stats_df = pd.DataFrame([
                {"æŒ‡æ¨™": k, "å€¤": v}
                for k, v in stats.items()
            ])
            section_content.extend([
                html.H4("ðŸ“Š åˆ†æžçµ±è¨ˆ"),
                _render_dataframe(stats_df, max_rows=20),
                html.Hr(),
            ])

    return section_content


def _render_mind_reader_from_cache(analysis_data: dict) -> html.Div:
    """Render mind reader analysis results from cached JSON data."""
    content = [
        html.H3("ðŸ§  AI / Mind Reader"),
        html.P("ã‚·ãƒ•ãƒˆä½œæˆè€…ã®æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹è§£èª­çµæžœã‚’è¡¨ç¤ºã—ã¾ã™ã€‚"),
        html.Hr(),
    ]

    # Add feature importance section
    if "feature_importance" in analysis_data:
        content.extend(_create_feature_importance_section(analysis_data["feature_importance"]))

    # Add other info sections
    content.extend(_create_mind_reader_info_sections(analysis_data))

    # JSON preview
    content.extend([
        html.H4("åˆ†æžçµæžœJSONï¼ˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼‰"),
        html.Pre(
            json.dumps(analysis_data, ensure_ascii=False, indent=2)[:2000] + "...",
            style={"backgroundColor": "#f5f5f5", "padding": "15px", "borderRadius": "5px", "fontSize": "12px", "overflow": "auto"}
        ),
    ])

    return html.Div(content)


# -----------------------------------------------------------------------------
# Gap Analysis page
# -----------------------------------------------------------------------------


def _create_gap_summary_section(gap_summary: pd.DataFrame) -> List:
    """Create Gap Summary section with KPI cards, chart, and table."""
    section_content = []

    # Calculate total and average gap
    total_gap = 0.0
    if 'total_gap_hours' in gap_summary.columns:
        total_gap = gap_summary['total_gap_hours'].sum()
    elif 'gap_hours' in gap_summary.columns:
        total_gap = gap_summary['gap_hours'].sum()

    avg_gap = total_gap / len(gap_summary) if len(gap_summary) > 0 else 0.0

    # KPI cards
    section_content.extend([
        html.H4("ðŸ“Š ä¹–é›¢ã‚µãƒžãƒª"),
        html.Div([
            _create_kpi_card("ç·ä¹–é›¢æ™‚é–“", f"{total_gap:.1f} h"),
            _create_kpi_card("å¹³å‡ä¹–é›¢", f"{avg_gap:.1f} h"),
            _create_kpi_card("åˆ†æžå¯¾è±¡", f"{len(gap_summary)} é …ç›®"),
        ], style={"display": "flex", "gap": "20px", "flexWrap": "wrap", "marginBottom": "20px"}),
        html.Hr(),
    ])

    # Bar chart for gap summary by role
    if 'role' in gap_summary.columns:
        gap_col = 'total_gap_hours' if 'total_gap_hours' in gap_summary.columns else \
                 'gap_hours' if 'gap_hours' in gap_summary.columns else gap_summary.columns[-1]

        fig = px.bar(
            gap_summary,
            x='role',
            y=gap_col,
            title="è·ç¨®åˆ¥ ä¹–é›¢æ™‚é–“",
            labels={'role': 'è·ç¨®', gap_col: 'ä¹–é›¢æ™‚é–“ (h)'},
            color=gap_col,
            color_continuous_scale='RdYlGn_r',  # Red for high gap
        )
        fig.update_layout(height=400)

        section_content.extend([
            html.H5("è·ç¨®åˆ¥ä¹–é›¢ã‚°ãƒ©ãƒ•"),
            dcc.Graph(figure=fig),
            html.Hr(),
        ])

    # Summary table
    section_content.extend([
        html.H5("ä¹–é›¢ã‚µãƒžãƒªãƒ†ãƒ¼ãƒ–ãƒ«"),
        _render_dataframe(gap_summary, max_rows=20),
        html.Hr(),
    ])

    return section_content


def _create_gap_heatmap_section(gap_heatmap: pd.DataFrame) -> List:
    """Create Gap Heatmap section with visualization and data preview."""
    section_content = [
        html.H4("ðŸ”¥ ä¹–é›¢ãƒ’ãƒ¼ãƒˆãƒžãƒƒãƒ—"),
        html.P("æ™‚é–“å¸¯åˆ¥ã®ç†æƒ³åŸºæº–ã‹ã‚‰ã®ä¹–é›¢åº¦ã‚’å¯è¦–åŒ–ã—ã¾ã™ã€‚"),
    ]

    # Create heatmap visualization
    try:
        # If gap_heatmap has multiple columns, use heatmap
        if len(gap_heatmap.columns) > 1:
            fig = px.imshow(
                gap_heatmap,
                aspect="auto",
                color_continuous_scale="RdBu_r",  # Red for positive gap (excess), Blue for negative (shortage)
                labels=dict(x="æ—¥ä»˜", y="æ™‚é–“å¸¯", color="ä¹–é›¢åº¦"),
                title="åŸºæº–ä¹–é›¢ãƒ’ãƒ¼ãƒˆãƒžãƒƒãƒ—"
            )
            fig.update_layout(height=500, margin=dict(l=20, r=20, t=60, b=20))
            section_content.append(dcc.Graph(figure=fig))
        else:
            # If single column, show as time series
            fig = px.line(
                gap_heatmap,
                x=gap_heatmap.index,
                y=gap_heatmap.columns[0],
                title="ä¹–é›¢åº¦ã®æŽ¨ç§»",
                labels={'x': 'æ™‚åˆ»', 'y': 'ä¹–é›¢åº¦'}
            )
            section_content.append(dcc.Graph(figure=fig))
    except Exception as e:
        section_content.append(html.P(f"ãƒ’ãƒ¼ãƒˆãƒžãƒƒãƒ—å¯è¦–åŒ–ã‚¨ãƒ©ãƒ¼: {str(e)}", style={"color": "#d9534f"}))

    # Data preview
    section_content.extend([
        html.Hr(),
        html.H5("ä¹–é›¢ãƒ’ãƒ¼ãƒˆãƒžãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼‰"),
        _render_dataframe(gap_heatmap.head(20), max_rows=20),
    ])

    return section_content


def page_gap_analysis(session: SessionData, metadata: Optional[dict]) -> html.Div:
    """Gap Analysis tab showing deviations from ideal standards."""
    scenario_name = metadata.get("scenario") if metadata else None
    _, scenario = session.get_scenario_data(scenario_name)

    # Load gap analysis data
    gap_summary = scenario.get_dataset("gap_summary")
    gap_heatmap = scenario.get_dataset("gap_heatmap")

    # Check if any data is available
    if (gap_summary is None or (isinstance(gap_summary, pd.DataFrame) and gap_summary.empty)) and \
       (gap_heatmap is None or (isinstance(gap_heatmap, pd.DataFrame) and gap_heatmap.empty)):
        # Create error message with helper function
        error_msg = create_missing_data_message(
            tab_name="ðŸ“‰ åŸºæº–ä¹–é›¢åˆ†æž (Gap Analysis)",
            required_files=["gap_summary.parquet", "gap_heatmap.parquet"],
            additional_info="ãƒ‡ãƒ¼ã‚¿ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸZIPã«å«ã¾ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
        )
        # Add explanation section
        explanation = html.Div([
            html.Hr(),
            html.Div([
                html.H4("åŸºæº–ä¹–é›¢åˆ†æžã¨ã¯"),
                html.P("ç†æƒ³çš„ãªäººå“¡é…ç½®åŸºæº–ã¨å®Ÿéš›ã®é…ç½®ã¨ã®ã‚®ãƒ£ãƒƒãƒ—ã‚’å¯è¦–åŒ–ã—ã¾ã™ã€‚"),
                html.Ul([
                    html.Li("gap_summary: è·ç¨®åˆ¥ã®ç·ä¹–é›¢æ™‚é–“ã‚µãƒžãƒª"),
                    html.Li("gap_heatmap: æ™‚é–“å¸¯åˆ¥ã®ä¹–é›¢åº¦ãƒ’ãƒ¼ãƒˆãƒžãƒƒãƒ—"),
                ]),
            ], style={"marginTop": "20px", "padding": "15px", "backgroundColor": "#f8f9fa", "borderRadius": "5px"}),
        ])
        return html.Div([error_msg, explanation])

    # Initialize content with header
    content = [
        html.H3("ðŸ“‰ åŸºæº–ä¹–é›¢åˆ†æž (Gap Analysis)"),
        html.P("ç†æƒ³åŸºæº–ã‹ã‚‰ã®ä¹–é›¢åº¦ã‚’æ¸¬å®šã—ã€æ”¹å–„ç›®æ¨™ã‚’æ˜Žç¢ºåŒ–ã—ã¾ã™ã€‚"),
        html.Hr(),
    ]

    # Add Gap Summary section
    if gap_summary is not None and isinstance(gap_summary, pd.DataFrame) and not gap_summary.empty:
        content.extend(_create_gap_summary_section(gap_summary))

    # Add Gap Heatmap section
    if gap_heatmap is not None and isinstance(gap_heatmap, pd.DataFrame) and not gap_heatmap.empty:
        content.extend(_create_gap_heatmap_section(gap_heatmap))

    # Check if any data sections were added
    if not content[3:]:
        content.append(html.P("è¡¨ç¤ºå¯èƒ½ãªä¹–é›¢ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"))

    return html.Div(content)


# -----------------------------------------------------------------------------
# Individual Analysis page
# -----------------------------------------------------------------------------


def page_individual(session: SessionData, metadata: Optional[dict]) -> html.Div:
    """Individual staff analysis tab showing work patterns and synergy."""
    scenario_name = metadata.get("scenario") if metadata else None
    _, scenario = session.get_scenario_data(scenario_name)

    # Check if we have long_df data
    long_df = scenario.get_dataset("long_df")
    if long_df is None or long_df.empty:
        return create_missing_data_message(
            tab_name="ðŸ‘¤ è·å“¡å€‹åˆ¥åˆ†æž",
            required_files=["intermediate_data.parquet (long_df)"],
            additional_info="ãƒ‡ãƒ¼ã‚¿ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸZIPã«å«ã¾ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
        )

    # Basic staff information
    if 'staff' not in long_df.columns:
        return html.Div([
            html.H3("ðŸ‘¤ è·å“¡å€‹åˆ¥åˆ†æž"),
            html.P("ãƒ‡ãƒ¼ã‚¿ã« staff ã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"),
        ])

    all_staff = sorted(long_df['staff'].unique().tolist())
    if not all_staff:
        return html.Div([
            html.H3("ðŸ‘¤ è·å“¡å€‹åˆ¥åˆ†æž"),
            html.P("è·å“¡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"),
        ])

    # For now, show summary of first staff member
    selected_staff = all_staff[0]
    staff_df = long_df[long_df['staff'] == selected_staff].copy()

    # Calculate basic info
    total_shifts = len(staff_df)
    total_hours = total_shifts * 0.5  # 30åˆ†å˜ä½

    unique_dates = 0
    if 'ds' in staff_df.columns:
        unique_dates = staff_df['ds'].dt.date.nunique()

    avg_hours_per_day = total_hours / unique_dates if unique_dates > 0 else 0.0

    top_codes = {}
    if 'code' in staff_df.columns:
        top_codes = staff_df['code'].value_counts().head(3).to_dict()

    return html.Div([
        html.H3(f"ðŸ‘¤ è·å“¡å€‹åˆ¥åˆ†æž"),
        html.P("å„è·å“¡ã®å‹¤å‹™ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨ã‚·ãƒŠã‚¸ãƒ¼åˆ†æžã‚’è¡¨ç¤ºã—ã¾ã™ã€‚"),
        html.Hr(),

        html.Div([
            html.H5(f"é¸æŠžè·å“¡: {selected_staff}"),
            html.P(f"ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¡¨ç¤º: {len(all_staff)}äººä¸­ã®æœ€åˆã®è·å“¡ï¼‰"),
        ], style={"marginBottom": "20px", "padding": "15px", "backgroundColor": "#f8f9fa", "borderRadius": "5px"}),

        html.Hr(),

        # Basic info cards
        html.H4("åŸºæœ¬æƒ…å ±"),
        html.Div([
            _create_kpi_card("ç·å‹¤å‹™æ—¥æ•°", f"{unique_dates} æ—¥"),
            _create_kpi_card("ç·å‹¤å‹™æ™‚é–“", f"{total_hours:.1f} h"),
            _create_kpi_card("å¹³å‡æ™‚é–“/æ—¥", f"{avg_hours_per_day:.1f} h"),
        ], style={"display": "flex", "gap": "20px", "flexWrap": "wrap", "marginBottom": "20px"}),

        html.Hr(),

        # Top work codes
        html.H4("ä¸»ãªå‹¤å‹™ã‚³ãƒ¼ãƒ‰"),
        html.Ul([
            html.Li(f"{code}: {count}å›ž")
            for code, count in top_codes.items()
        ]) if top_codes else html.P("å‹¤å‹™ã‚³ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“"),

        html.Hr(),

        html.P("âš ï¸ æ³¨æ„: å®Œå…¨ãªã‚·ãƒŠã‚¸ãƒ¼åˆ†æžæ©Ÿèƒ½ã¯ shift_suite.tasks.analyzers.synergy ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒå¿…è¦ã§ã™ã€‚",
               style={"color": "#856404", "backgroundColor": "#fff3cd", "padding": "15px", "borderRadius": "5px"}),
    ])


# -----------------------------------------------------------------------------
# Team Analysis page
# -----------------------------------------------------------------------------


def page_team(session: SessionData, metadata: Optional[dict]) -> html.Div:
    """Team analysis tab showing dynamic team detection and collaboration patterns."""
    scenario_name = metadata.get("scenario") if metadata else None
    _, scenario = session.get_scenario_data(scenario_name)

    # Check if we have long_df data
    long_df = scenario.get_dataset("long_df")
    if long_df is None or long_df.empty:
        return create_missing_data_message(
            tab_name="ðŸ‘¥ ãƒãƒ¼ãƒ åˆ†æž",
            required_files=["intermediate_data.parquet (long_df)"],
            additional_info="ãƒ‡ãƒ¼ã‚¿ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸZIPã«å«ã¾ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
        )

    # Basic team information
    if 'staff' not in long_df.columns:
        return html.Div([
            html.H3("ðŸ‘¥ ãƒãƒ¼ãƒ åˆ†æž"),
            html.P("ãƒ‡ãƒ¼ã‚¿ã« staff ã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"),
        ])

    total_staff = long_df['staff'].nunique()

    # Calculate co-working patterns
    unique_dates = 0
    if 'ds' in long_df.columns:
        unique_dates = long_df['ds'].dt.date.nunique()

    return html.Div([
        html.H3("ðŸ‘¥ ãƒãƒ¼ãƒ åˆ†æž"),
        html.P("å‹•çš„ãƒãƒ¼ãƒ æŠ½å‡ºã¨ãƒãƒ¼ãƒ å‹•æ…‹åˆ†æžã‚’è¡¨ç¤ºã—ã¾ã™ã€‚"),
        html.Hr(),

        html.Div([
            html.H4("ãƒãƒ¼ãƒ åˆ†æžã®æ©Ÿèƒ½"),
            html.Ul([
                html.Li("å…±åƒé–¢ä¿‚ã®æ¤œå‡º: å®Ÿéš›ã«ä¸€ç·’ã«åƒã„ã¦ã„ã‚‹è·å“¡ã®çµ„ã¿åˆã‚ã›ã‚’ç‰¹å®š"),
                html.Li("ãƒãƒ¼ãƒ çµæŸåº¦: ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£æ¤œå‡ºã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ãƒãƒ¼ãƒ ã‚’è‡ªå‹•æŠ½å‡º"),
                html.Li("ã‚·ãƒŠã‚¸ãƒ¼åˆ†æž: ãƒãƒ¼ãƒ å†…ã®ç›¸æ€§ã¨å”åŠ›ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å¯è¦–åŒ–"),
            ]),
        ], style={"marginBottom": "20px", "padding": "15px", "backgroundColor": "#f8f9fa", "borderRadius": "5px"}),

        html.Hr(),

        html.H4("ãƒ‡ãƒ¼ã‚¿ã‚µãƒžãƒª"),
        html.Div([
            _create_kpi_card("ç·è·å“¡æ•°", f"{total_staff} äºº"),
            _create_kpi_card("åˆ†æžæ—¥æ•°", f"{unique_dates} æ—¥"),
            _create_kpi_card("ç·ã‚·ãƒ•ãƒˆæ•°", f"{len(long_df):,} è¡Œ"),
        ], style={"display": "flex", "gap": "20px", "flexWrap": "wrap", "marginBottom": "20px"}),

        html.Hr(),

        html.P("âš ï¸ æ³¨æ„: å®Œå…¨ãªãƒãƒ¼ãƒ åˆ†æžæ©Ÿèƒ½ã¯ shift_suite.tasks.analyzers.team_dynamics ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒå¿…è¦ã§ã™ã€‚",
               style={"color": "#856404", "backgroundColor": "#fff3cd", "padding": "15px", "borderRadius": "5px"}),
    ])


# -----------------------------------------------------------------------------
# Fatigue Analysis page
# -----------------------------------------------------------------------------


def _create_fatigue_summary_section(fatigue_score: pd.DataFrame) -> List:
    """Create fatigue summary KPI cards and TOP20 chart."""
    section_content = []

    if 'total_fatigue' not in fatigue_score.columns:
        return section_content

    # Calculate summary statistics
    avg_fatigue = fatigue_score['total_fatigue'].mean()
    max_fatigue = fatigue_score['total_fatigue'].max()
    high_risk_count = len(fatigue_score[fatigue_score['total_fatigue'] > 70])

    section_content.extend([
        html.H4("ç–²åŠ´ã‚¹ã‚³ã‚¢ã‚µãƒžãƒª"),
        html.Div([
            _create_kpi_card("å¹³å‡ç–²åŠ´ã‚¹ã‚³ã‚¢", f"{avg_fatigue:.1f}"),
            _create_kpi_card("æœ€å¤§ç–²åŠ´ã‚¹ã‚³ã‚¢", f"{max_fatigue:.1f}"),
            _create_kpi_card("é«˜ãƒªã‚¹ã‚¯è·å“¡", f"{high_risk_count} äºº"),
        ], style={"display": "flex", "gap": "20px", "flexWrap": "wrap", "marginBottom": "20px"}),
        html.Hr(),
    ])

    # Top fatigued staff chart
    top_fatigued = fatigue_score.nlargest(20, 'total_fatigue')

    fig = px.bar(
        top_fatigued,
        x='staff' if 'staff' in top_fatigued.columns else top_fatigued.index,
        y='total_fatigue',
        title="ç–²åŠ´ã‚¹ã‚³ã‚¢ TOP20",
        labels={'staff': 'è·å“¡', 'total_fatigue': 'ç·åˆç–²åŠ´ã‚¹ã‚³ã‚¢'},
        color='total_fatigue',
        color_continuous_scale='Reds',
    )
    fig.update_layout(height=400)

    section_content.extend([
        html.H4("ç–²åŠ´ã‚¹ã‚³ã‚¢ TOP20"),
        dcc.Graph(figure=fig),
        html.Hr(),
    ])

    return section_content


def page_fatigue(session: SessionData, metadata: Optional[dict]) -> html.Div:
    """Fatigue analysis tab showing 6-element fatigue evaluation."""
    scenario_name = metadata.get("scenario") if metadata else None
    _, scenario = session.get_scenario_data(scenario_name)

    # Try to load fatigue score data
    fatigue_score = scenario.get_dataset("fatigue_score")

    if fatigue_score is None or (isinstance(fatigue_score, pd.DataFrame) and fatigue_score.empty):
        # Create error message with helper function
        error_msg = create_missing_data_message(
            tab_name="ðŸ˜´ ç–²åŠ´åˆ†æž",
            required_files=["fatigue_score.parquet"],
            additional_info="ãƒ‡ãƒ¼ã‚¿ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸZIPã«å«ã¾ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
        )
        # Add explanation section
        explanation = html.Div([
            html.Hr(),
            html.Div([
                html.H4("ç–²åŠ´åˆ†æžã®6è¦ç´ "),
                html.Ol([
                    html.Li("é€£ç¶šå‹¤å‹™ç–²åŠ´: é€£ç¶šã—ã¦åƒã„ãŸæ—¥æ•°ã«ã‚ˆã‚‹ç–²åŠ´"),
                    html.Li("å¤œå‹¤ç–²åŠ´: å¤œé–“å‹¤å‹™ã«ã‚ˆã‚‹ç”Ÿä½“ãƒªã‚ºãƒ ã¸ã®å½±éŸ¿"),
                    html.Li("é•·æ™‚é–“å‹¤å‹™ç–²åŠ´: 1æ—¥ã®åŠ´åƒæ™‚é–“ã«ã‚ˆã‚‹ç–²åŠ´"),
                    html.Li("ä¼‘æ†©ä¸è¶³ç–²åŠ´: é©åˆ‡ãªä¼‘æ†©ãŒå–ã‚Œã¦ã„ãªã„å ´åˆã®ç–²åŠ´"),
                    html.Li("ä¸è¦å‰‡å‹¤å‹™ç–²åŠ´: å‹¤å‹™æ™‚é–“å¸¯ãŒä¸è¦å‰‡ãªå ´åˆã®ç–²åŠ´"),
                    html.Li("ç·åˆç–²åŠ´ã‚¹ã‚³ã‚¢: ä¸Šè¨˜5è¦ç´ ã‚’çµ±åˆã—ãŸç·åˆæŒ‡æ¨™"),
                ]),
            ], style={"marginTop": "20px", "padding": "15px", "backgroundColor": "#f8f9fa", "borderRadius": "5px"}),
        ])
        return html.Div([error_msg, explanation])

    # Display fatigue data
    content = [
        html.H3("ðŸ˜´ ç–²åŠ´åˆ†æž"),
        html.P("6è¦ç´ ç–²åŠ´è©•ä¾¡ã«åŸºã¥ãè·å“¡ã®ç–²åŠ´åº¦ã‚’åˆ†æžã—ã¾ã™ã€‚"),
        html.Hr(),
    ]

    # Add summary section
    content.extend(_create_fatigue_summary_section(fatigue_score))

    # Data table
    content.extend([
        html.H4("ç–²åŠ´ã‚¹ã‚³ã‚¢è©³ç´°ãƒ‡ãƒ¼ã‚¿"),
        _render_dataframe(fatigue_score, max_rows=20),
    ])

    return html.Div(content)


# -----------------------------------------------------------------------------
# Leave Analysis page
# -----------------------------------------------------------------------------


def page_leave(session: SessionData, metadata: Optional[dict]) -> html.Div:
    """Leave analysis tab showing vacation/leave patterns."""
    scenario_name = metadata.get("scenario") if metadata else None
    _, scenario = session.get_scenario_data(scenario_name)

    # Try to load leave analysis data
    leave_analysis = scenario.get_dataset("leave_analysis")
    leave_ratio = scenario.get_dataset("leave_ratio_breakdown")

    if (leave_analysis is None or (isinstance(leave_analysis, pd.DataFrame) and leave_analysis.empty)) and \
       (leave_ratio is None or (isinstance(leave_ratio, pd.DataFrame) and leave_ratio.empty)):
        # Create error message with helper function
        error_msg = create_missing_data_message(
            tab_name="ðŸ–ï¸ ä¼‘æš‡åˆ†æž",
            required_files=["leave_analysis.csv", "leave_ratio_breakdown.csv"],
            additional_info="ãƒ‡ãƒ¼ã‚¿ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸZIPã«å«ã¾ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
        )
        # Add explanation section
        explanation = html.Div([
            html.Hr(),
            html.Div([
                html.H4("ä¼‘æš‡åˆ†æžã®ç›®çš„"),
                html.P("è·å“¡ã®ä¼‘æš‡å–å¾—ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æžã—ã€ãƒ¯ãƒ¼ã‚¯ãƒ©ã‚¤ãƒ•ãƒãƒ©ãƒ³ã‚¹ã®æ”¹å–„ã«å½¹ç«‹ã¦ã¾ã™ã€‚"),
                html.Ul([
                    html.Li("ä¼‘æš‡å–å¾—çŽ‡ã®å¯è¦–åŒ–"),
                    html.Li("ä¼‘æš‡å–å¾—ãŒå°‘ãªã„è·å“¡ã®ç‰¹å®š"),
                    html.Li("ä¼‘æš‡ã‚¿ã‚¤ãƒ—åˆ¥ï¼ˆæœ‰çµ¦/ç‰¹åˆ¥/ç—…æ°—ï¼‰ã®åˆ†æž"),
                    html.Li("æœˆåˆ¥ä¼‘æš‡å–å¾—æŽ¨ç§»ã®æŠŠæ¡"),
                ]),
            ], style={"marginTop": "20px", "padding": "15px", "backgroundColor": "#f8f9fa", "borderRadius": "5px"}),
        ])
        return html.Div([error_msg, explanation])

    content = [
        html.H3("ðŸ–ï¸ ä¼‘æš‡åˆ†æž"),
        html.P("è·å“¡ã®ä¼‘æš‡å–å¾—ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æžã—ã¾ã™ã€‚"),
        html.Hr(),
    ]

    # Leave analysis data
    if leave_analysis is not None and isinstance(leave_analysis, pd.DataFrame) and not leave_analysis.empty:
        content.extend([
            html.H4("ä¼‘æš‡åˆ†æžçµæžœ"),
            _render_dataframe(leave_analysis, max_rows=20),
            html.Hr(),
        ])

    # Leave ratio breakdown
    if leave_ratio is not None and isinstance(leave_ratio, pd.DataFrame) and not leave_ratio.empty:
        content.extend([
            html.H4("ä¼‘æš‡æ¯”çŽ‡å†…è¨³"),
            _render_dataframe(leave_ratio, max_rows=20),
            html.Hr(),
        ])

    if len(content) == 3:  # Only header content
        content.append(html.P("è¡¨ç¤ºå¯èƒ½ãªä¼‘æš‡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"))

    return html.Div(content)


# -----------------------------------------------------------------------------
# Fairness Analysis page
# -----------------------------------------------------------------------------


def page_fairness(session: SessionData, metadata: Optional[dict]) -> html.Div:
    """Fairness analysis tab showing work hour distribution and Jain's Index."""
    scenario_name = metadata.get("scenario") if metadata else None
    _, scenario = session.get_scenario_data(scenario_name)

    # Try to load fairness data
    fairness_before = scenario.get_dataset("fairness_before")
    fairness_after = scenario.get_dataset("fairness_after")

    if (fairness_before is None or (isinstance(fairness_before, pd.DataFrame) and fairness_before.empty)) and \
       (fairness_after is None or (isinstance(fairness_after, pd.DataFrame) and fairness_after.empty)):
        # Create error message with helper function
        error_msg = create_missing_data_message(
            tab_name="âš–ï¸ å…¬å¹³æ€§åˆ†æž",
            required_files=["fairness_before.parquet", "fairness_after.parquet"],
            additional_info="ãƒ‡ãƒ¼ã‚¿ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸZIPã«å«ã¾ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
        )
        # Add explanation section
        explanation = html.Div([
            html.Hr(),
            html.Div([
                html.H4("å…¬å¹³æ€§åˆ†æžã«ã¤ã„ã¦"),
                html.P("Jain's Index ã‚’ç”¨ã„ã¦å‹¤å‹™æ™‚é–“ã®å…¬å¹³æ€§ã‚’è©•ä¾¡ã—ã¾ã™ã€‚"),
                html.Ul([
                    html.Li("Jain's Index: 0~1ã®å€¤ã§ã€1ã«è¿‘ã„ã»ã©å…¬å¹³"),
                    html.Li("è·å“¡é–“ã®å‹¤å‹™æ™‚é–“åˆ†å¸ƒã‚’å¯è¦–åŒ–"),
                    html.Li("æ”¹å–„å‰å¾Œã®æ¯”è¼ƒãŒå¯èƒ½"),
                    html.Li("ä¸å…¬å¹³ãªé…ç½®ã®æ¤œå‡ºã¨æ”¹å–„ææ¡ˆ"),
                ]),
                html.Hr(),
                html.H5("Jain's Index ã®è¨ˆç®—å¼"),
                html.P("J = (Î£x_i)Â² / (n * Î£x_iÂ²)"),
                html.P("x_i: å„è·å“¡ã®å‹¤å‹™æ™‚é–“, n: è·å“¡æ•°", style={"fontSize": "12px", "color": "#666"}),
            ], style={"marginTop": "20px", "padding": "15px", "backgroundColor": "#f8f9fa", "borderRadius": "5px"}),
        ])
        return html.Div([error_msg, explanation])

    content = [
        html.H3("âš–ï¸ å…¬å¹³æ€§åˆ†æž"),
        html.P("å‹¤å‹™æ™‚é–“ã®å…¬å¹³æ€§ã‚’ Jain's Index ã§è©•ä¾¡ã—ã¾ã™ã€‚"),
        html.Hr(),
    ]

    # Fairness before
    if fairness_before is not None and isinstance(fairness_before, pd.DataFrame) and not fairness_before.empty:
        jain_before = _calculate_jain_index(fairness_before)

        content.extend([
            html.H4("æ”¹å–„å‰ã®å…¬å¹³æ€§"),
            html.Div([
                _create_kpi_card("Jain's Index", f"{jain_before:.3f}"),
                _create_kpi_card("å¯¾è±¡è·å“¡æ•°", f"{len(fairness_before)} äºº"),
            ], style={"display": "flex", "gap": "20px", "flexWrap": "wrap", "marginBottom": "20px"}),
            _render_dataframe(fairness_before, max_rows=10),
            html.Hr(),
        ])

    # Fairness after
    if fairness_after is not None and isinstance(fairness_after, pd.DataFrame) and not fairness_after.empty:
        jain_after = _calculate_jain_index(fairness_after)

        improvement = ""
        if fairness_before is not None and not fairness_before.empty:
            jain_before_val = _calculate_jain_index(fairness_before)
            improvement = f" (æ”¹å–„: +{(jain_after - jain_before_val):.3f})"

        content.extend([
            html.H4("æ”¹å–„å¾Œã®å…¬å¹³æ€§"),
            html.Div([
                _create_kpi_card("Jain's Index", f"{jain_after:.3f}{improvement}"),
                _create_kpi_card("å¯¾è±¡è·å“¡æ•°", f"{len(fairness_after)} äºº"),
            ], style={"display": "flex", "gap": "20px", "flexWrap": "wrap", "marginBottom": "20px"}),
            _render_dataframe(fairness_after, max_rows=10),
            html.Hr(),
        ])

    if len(content) == 3:  # Only header content
        content.append(html.P("è¡¨ç¤ºå¯èƒ½ãªå…¬å¹³æ€§ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"))

    return html.Div(content)


def _calculate_jain_index(df: pd.DataFrame) -> float:
    """Calculate Jain's Index for fairness evaluation."""
    if df.empty:
        return 0.0

    # Try common column names for work hours
    work_hours_col = None
    for col in ['work_hours', 'total_hours', 'hours', 'total_work_hours']:
        if col in df.columns:
            work_hours_col = col
            break

    if work_hours_col is None:
        # Try first numeric column
        numeric_cols = df.select_dtypes(include=['number']).columns
        if len(numeric_cols) > 0:
            work_hours_col = numeric_cols[0]
        else:
            return 0.0

    values = df[work_hours_col].values
    n = len(values)
    if n == 0:
        return 0.0

    numerator = (values.sum()) ** 2
    denominator = n * (values ** 2).sum()

    return float(numerator / denominator) if denominator > 0 else 0.0


# -----------------------------------------------------------------------------
# Optimization Analysis page
# -----------------------------------------------------------------------------


def page_optimization(session: SessionData, metadata: Optional[dict]) -> html.Div:
    """Optimization analysis tab showing optimization score time series."""
    scenario_name = metadata.get("scenario") if metadata else None
    _, scenario = session.get_scenario_data(scenario_name)

    # Try to load optimization score data
    opt_score = scenario.get_dataset("optimization_score_time")

    if opt_score is None or (isinstance(opt_score, pd.DataFrame) and opt_score.empty):
        # Create error message with helper function
        error_msg = create_missing_data_message(
            tab_name="ðŸŽ¯ æœ€é©åŒ–åˆ†æž",
            required_files=["optimization_score_time.parquet"],
            additional_info="ãƒ‡ãƒ¼ã‚¿ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸZIPã«å«ã¾ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
        )
        # Add explanation section
        explanation = html.Div([
            html.Hr(),
            html.Div([
                html.H4("æœ€é©åŒ–åˆ†æžã«ã¤ã„ã¦"),
                html.P("ã‚·ãƒ•ãƒˆæœ€é©åŒ–ã®ã‚¹ã‚³ã‚¢ã‚’æ™‚ç³»åˆ—ã§è¡¨ç¤ºã—ã€æœ€é©åŒ–ã®åŠ¹æžœã‚’æ¸¬å®šã—ã¾ã™ã€‚"),
                html.Ul([
                    html.Li("æ™‚ç³»åˆ—ã§ã®æœ€é©åŒ–ã‚¹ã‚³ã‚¢æŽ¨ç§»"),
                    html.Li("ãƒ”ãƒ¼ã‚¯æ™‚ã®ç‰¹å®š"),
                    html.Li("æ”¹å–„ãƒˆãƒ¬ãƒ³ãƒ‰ã®å¯è¦–åŒ–"),
                ]),
            ], style={"marginTop": "20px", "padding": "15px", "backgroundColor": "#f8f9fa", "borderRadius": "5px"}),
        ])
        return html.Div([error_msg, explanation])

    # Display optimization score data
    content = [
        html.H3("ðŸŽ¯ æœ€é©åŒ–åˆ†æž"),
        html.P("ã‚·ãƒ•ãƒˆæœ€é©åŒ–ã®ã‚¹ã‚³ã‚¢ã‚’æ™‚ç³»åˆ—ã§è¡¨ç¤ºã—ã¾ã™ã€‚"),
        html.Hr(),
    ]

    # Time series graph
    try:
        fig = px.line(
            opt_score,
            x=opt_score.index if hasattr(opt_score, 'index') else range(len(opt_score)),
            y=opt_score.columns[0] if len(opt_score.columns) > 0 else None,
            title="æœ€é©åŒ–ã‚¹ã‚³ã‚¢ã®æŽ¨ç§»",
            labels={'x': 'æ—¥ä»˜', 'y': 'ã‚¹ã‚³ã‚¢'}
        )
        fig.update_layout(height=400)

        content.extend([
            html.H4("ã‚¹ã‚³ã‚¢æŽ¨ç§»"),
            dcc.Graph(figure=fig),
            html.Hr(),
        ])
    except Exception as e:
        content.append(html.P(f"ã‚°ãƒ©ãƒ•ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}", style={"color": "#d9534f"}))

    # Data table
    content.extend([
        html.H4("è©³ç´°ãƒ‡ãƒ¼ã‚¿"),
        _render_dataframe(opt_score, max_rows=20),
    ])

    return html.Div(content)


# -----------------------------------------------------------------------------
# Forecast Analysis page
# -----------------------------------------------------------------------------


def page_forecast(session: SessionData, metadata: Optional[dict]) -> html.Div:
    """Forecast analysis tab showing demand prediction results."""
    scenario_name = metadata.get("scenario") if metadata else None
    _, scenario = session.get_scenario_data(scenario_name)

    # Try to load forecast data
    forecast_df = scenario.get_dataset("forecast")
    forecast_json = scenario.get_dataset("forecast_json")
    demand_series = scenario.get_dataset("demand_series")

    if (forecast_df is None or (isinstance(forecast_df, pd.DataFrame) and forecast_df.empty)) and \
       (forecast_json is None) and \
       (demand_series is None or (isinstance(demand_series, pd.DataFrame) and demand_series.empty)):
        # Create error message with helper function
        error_msg = create_missing_data_message(
            tab_name="ðŸ“ˆ éœ€è¦äºˆæ¸¬",
            required_files=["forecast.parquet", "forecast.json", "demand_series.csv"],
            additional_info="ãƒ‡ãƒ¼ã‚¿ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸZIPã«å«ã¾ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
        )
        # Add explanation section
        explanation = html.Div([
            html.Hr(),
            html.Div([
                html.H4("éœ€è¦äºˆæ¸¬ã«ã¤ã„ã¦"),
                html.P("å°†æ¥ã®äººå“¡éœ€è¦ã‚’äºˆæ¸¬ã—ã€äº‹å‰ã®äººå“¡è¨ˆç”»ã«å½¹ç«‹ã¦ã¾ã™ã€‚"),
                html.Ul([
                    html.Li("æ™‚ç³»åˆ—éœ€è¦äºˆæ¸¬"),
                    html.Li("å­£ç¯€æ€§ãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æž"),
                    html.Li("äºˆæ¸¬ä¿¡é ¼åŒºé–“"),
                ]),
            ], style={"marginTop": "20px", "padding": "15px", "backgroundColor": "#f8f9fa", "borderRadius": "5px"}),
        ])
        return html.Div([error_msg, explanation])

    content = [
        html.H3("ðŸ“ˆ éœ€è¦äºˆæ¸¬"),
        html.P("å°†æ¥ã®äººå“¡éœ€è¦ã‚’äºˆæ¸¬ã—ã¾ã™ã€‚"),
        html.Hr(),
    ]

    # Demand series graph
    if demand_series is not None and isinstance(demand_series, pd.DataFrame) and not demand_series.empty:
        try:
            fig = px.line(
                demand_series,
                x=demand_series.index if hasattr(demand_series, 'index') else range(len(demand_series)),
                y=demand_series.columns[0] if len(demand_series.columns) > 0 else None,
                title="éœ€è¦ç³»åˆ—",
                labels={'x': 'æ™‚åˆ»', 'y': 'éœ€è¦'}
            )
            fig.update_layout(height=400)

            content.extend([
                html.H4("éœ€è¦ç³»åˆ—"),
                dcc.Graph(figure=fig),
                html.Hr(),
            ])
        except Exception:
            pass

    # Forecast results table
    if forecast_df is not None and isinstance(forecast_df, pd.DataFrame) and not forecast_df.empty:
        content.extend([
            html.H4("äºˆæ¸¬çµæžœ"),
            _render_dataframe(forecast_df, max_rows=20),
            html.Hr(),
        ])

    if len(content) == 3:  # Only header content
        content.append(html.P("è¡¨ç¤ºå¯èƒ½ãªäºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"))

    return html.Div(content)


# -----------------------------------------------------------------------------
# Hire Plan page
# -----------------------------------------------------------------------------


def page_hire_plan(session: SessionData, metadata: Optional[dict]) -> html.Div:
    """Hire plan tab showing recruitment planning."""
    scenario_name = metadata.get("scenario") if metadata else None
    _, scenario = session.get_scenario_data(scenario_name)

    # Try to load hire plan data
    hire_plan = scenario.get_dataset("hire_plan")

    if hire_plan is None or (isinstance(hire_plan, pd.DataFrame) and hire_plan.empty):
        # Create error message with helper function
        error_msg = create_missing_data_message(
            tab_name="ðŸ‘” æŽ¡ç”¨è¨ˆç”»",
            required_files=["hire_plan.parquet"],
            additional_info="ãƒ‡ãƒ¼ã‚¿ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸZIPã«å«ã¾ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
        )
        # Add explanation section
        explanation = html.Div([
            html.Hr(),
            html.Div([
                html.H4("æŽ¡ç”¨è¨ˆç”»ã«ã¤ã„ã¦"),
                html.P("ä¸è¶³ã‚’è§£æ¶ˆã™ã‚‹ãŸã‚ã®æŽ¡ç”¨è¨ˆç”»ã‚’æç¤ºã—ã€æŽ¡ç”¨äººæ•°ã®æ ¹æ‹ ã‚’ç¤ºã—ã¾ã™ã€‚"),
                html.Ul([
                    html.Li("è·ç¨®åˆ¥æŽ¡ç”¨æŽ¨å¥¨äººæ•°"),
                    html.Li("å„ªå…ˆåº¦é †æŽ¡ç”¨è¨ˆç”»"),
                    html.Li("ã‚³ã‚¹ãƒˆè©¦ç®—"),
                ]),
            ], style={"marginTop": "20px", "padding": "15px", "backgroundColor": "#f8f9fa", "borderRadius": "5px"}),
        ])
        return html.Div([error_msg, explanation])

    content = [
        html.H3("ðŸ‘” æŽ¡ç”¨è¨ˆç”»"),
        html.P("ä¸è¶³ã‚’è§£æ¶ˆã™ã‚‹ãŸã‚ã®æŽ¡ç”¨è¨ˆç”»ã‚’æç¤ºã—ã¾ã™ã€‚"),
        html.Hr(),
    ]

    # Display hire plan data
    content.extend([
        html.H4("æŽ¡ç”¨è¨ˆç”»ãƒ‡ãƒ¼ã‚¿"),
        _render_dataframe(hire_plan, max_rows=20),
    ])

    return html.Div(content)


# -----------------------------------------------------------------------------
# Cost Analysis page
# -----------------------------------------------------------------------------


def page_cost(session: SessionData, metadata: Optional[dict]) -> html.Div:
    """Cost analysis tab showing labor cost visualization."""
    scenario_name = metadata.get("scenario") if metadata else None
    _, scenario = session.get_scenario_data(scenario_name)

    # Try to load cost data
    daily_cost = scenario.get_dataset("daily_cost")
    cost_benefit = scenario.get_dataset("cost_benefit")

    if (daily_cost is None or (isinstance(daily_cost, pd.DataFrame) and daily_cost.empty)) and \
       (cost_benefit is None or (isinstance(cost_benefit, pd.DataFrame) and cost_benefit.empty)):
        # Create error message with helper function
        error_msg = create_missing_data_message(
            tab_name="ðŸ’° ã‚³ã‚¹ãƒˆåˆ†æž",
            required_files=["daily_cost.parquet", "cost_benefit.parquet"],
            additional_info="ãƒ‡ãƒ¼ã‚¿ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸZIPã«å«ã¾ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
        )
        # Add explanation section
        explanation = html.Div([
            html.Hr(),
            html.Div([
                html.H4("ã‚³ã‚¹ãƒˆåˆ†æžã«ã¤ã„ã¦"),
                html.P("äººä»¶è²»ã‚’å¯è¦–åŒ–ã—ã€ã‚³ã‚¹ãƒˆæœ€é©åŒ–ã®æ ¹æ‹ ã‚’æä¾›ã—ã¾ã™ã€‚"),
                html.Ul([
                    html.Li("æ—¥æ¬¡ã‚³ã‚¹ãƒˆæŽ¨ç§»"),
                    html.Li("ã‚³ã‚¹ãƒˆåŠ¹æžœåˆ†æž"),
                    html.Li("ROIè¨ˆç®—"),
                ]),
            ], style={"marginTop": "20px", "padding": "15px", "backgroundColor": "#f8f9fa", "borderRadius": "5px"}),
        ])
        return html.Div([error_msg, explanation])

    content = [
        html.H3("ðŸ’° ã‚³ã‚¹ãƒˆåˆ†æž"),
        html.P("äººä»¶è²»ã¨ã‚³ã‚¹ãƒˆåŠ¹æžœã‚’åˆ†æžã—ã¾ã™ã€‚"),
        html.Hr(),
    ]

    # Daily cost graph
    if daily_cost is not None and isinstance(daily_cost, pd.DataFrame) and not daily_cost.empty:
        try:
            fig = px.line(
                daily_cost,
                x=daily_cost.index if hasattr(daily_cost, 'index') else range(len(daily_cost)),
                y=daily_cost.columns[0] if len(daily_cost.columns) > 0 else None,
                title="æ—¥æ¬¡ã‚³ã‚¹ãƒˆæŽ¨ç§»",
                labels={'x': 'æ—¥ä»˜', 'y': 'ã‚³ã‚¹ãƒˆ (å††)'}
            )
            fig.update_layout(height=400)

            content.extend([
                html.H4("æ—¥æ¬¡ã‚³ã‚¹ãƒˆ"),
                dcc.Graph(figure=fig),
                html.Hr(),
            ])
        except Exception:
            pass

    # Cost benefit analysis
    if cost_benefit is not None and isinstance(cost_benefit, pd.DataFrame) and not cost_benefit.empty:
        content.extend([
            html.H4("ã‚³ã‚¹ãƒˆåŠ¹æžœåˆ†æž"),
            _render_dataframe(cost_benefit, max_rows=20),
            html.Hr(),
        ])

    if len(content) == 3:  # Only header content
        content.append(html.P("è¡¨ç¤ºå¯èƒ½ãªã‚³ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"))

    return html.Div(content)


# -----------------------------------------------------------------------------
# Summary Report page
# -----------------------------------------------------------------------------


def _collect_summary_data(scenario: ScenarioData) -> Dict[str, Any]:
    """Collect all summary metrics and findings for the summary report."""
    summary_data = {}

    # Basic metrics
    summary_data['total_shortage_hours'] = _calculate_total_shortage_hours(scenario.shortage_time)
    summary_data['total_staff'] = len(scenario.roles) if scenario.roles else 0

    # Date range
    if not scenario.heat_staff.empty and len(scenario.heat_staff.columns) > 0:
        summary_data['date_range'] = f"{scenario.heat_staff.columns[0]} ~ {scenario.heat_staff.columns[-1]}"
    else:
        summary_data['date_range'] = "N/A"

    # Fairness score
    fairness_before = scenario.get_dataset("fairness_before")
    if fairness_before is not None and isinstance(fairness_before, pd.DataFrame) and not fairness_before.empty:
        summary_data['fairness_score'] = _calculate_jain_index(fairness_before)
    else:
        summary_data['fairness_score'] = None

    # Top fatigued staff
    top_fatigue_staff = []
    fatigue_score = scenario.get_dataset("fatigue_score")
    if fatigue_score is not None and isinstance(fatigue_score, pd.DataFrame) and not fatigue_score.empty:
        if 'total_fatigue' in fatigue_score.columns:
            top3 = fatigue_score.nlargest(3, 'total_fatigue')
            top_fatigue_staff = top3['staff'].tolist() if 'staff' in top3.columns else top3.index.tolist()
    summary_data['top_fatigue_staff'] = top_fatigue_staff

    # Key findings from blueprint
    key_findings = []
    blueprint_data = _blueprint_analysis_for_scenario(scenario)
    if blueprint_data and 'rules_df' in blueprint_data:
        rules_df = _ensure_dataframe(blueprint_data['rules_df'])
        if not rules_df.empty and 'ç™ºè¦‹ã•ã‚ŒãŸæ³•å‰‡' in rules_df.columns:
            strength_col = 'æ³•å‰‡ã®å¼·åº¦' if 'æ³•å‰‡ã®å¼·åº¦' in rules_df.columns else rules_df.columns[0]
            top_rules = rules_df.nlargest(3, strength_col) if strength_col in rules_df.columns else rules_df.head(3)
            key_findings = top_rules['ç™ºè¦‹ã•ã‚ŒãŸæ³•å‰‡'].tolist()
    summary_data['key_findings'] = key_findings

    return summary_data


def page_summary(session: SessionData, metadata: Optional[dict]) -> html.Div:
    """Summary report tab showing integrated analysis results."""
    scenario_name = metadata.get("scenario") if metadata else None
    _, scenario = session.get_scenario_data(scenario_name)

    # Collect all summary data
    data = _collect_summary_data(scenario)

    # Build content
    content = [
        html.H3("ðŸ“„ çµ±åˆã‚µãƒžãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆ"),
        html.P("å…¨åˆ†æžçµæžœã®çµ±åˆãƒ¬ãƒãƒ¼ãƒˆã§ã™ã€‚"),
        html.Hr(),

        # Executive Summary
        html.H4("ðŸŽ¯ ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒžãƒª"),
        html.Div([
            _create_kpi_card("ç·ä¸è¶³æ™‚é–“", f"{data['total_shortage_hours']:.1f} h"),
            _create_kpi_card("ç·è·å“¡æ•°", f"{data['total_staff']} äºº"),
            _create_kpi_card("åˆ†æžæœŸé–“", data['date_range']),
            _create_kpi_card("å…¬å¹³æ€§", f"{data['fairness_score']:.2f}" if data['fairness_score'] else "N/A"),
        ], style={"display": "flex", "gap": "20px", "flexWrap": "wrap", "marginBottom": "20px"}),

        html.Hr(),
    ]

    # Key findings
    if data['key_findings']:
        content.extend([
            html.H4("ðŸ” ä¸»è¦ãªç™ºè¦‹äº‹é …"),
            html.Ul([html.Li(finding) for finding in data['key_findings']]),
            html.Hr(),
        ])

    # Recommendations
    recommendations = []
    if data['total_shortage_hours'] > 100:
        recommendations.append("âš ï¸ ç·ä¸è¶³æ™‚é–“ãŒ100æ™‚é–“ã‚’è¶…ãˆã¦ã„ã¾ã™ã€‚æŽ¡ç”¨ã‚’æ¤œè¨Žã—ã¦ãã ã•ã„ã€‚")
    if data['fairness_score'] and data['fairness_score'] < 0.8:
        recommendations.append("âš ï¸ å…¬å¹³æ€§ã‚¹ã‚³ã‚¢ãŒä½Žã„ã§ã™ã€‚å‹¤å‹™æ™‚é–“ã®å‡ç­‰åŒ–ã‚’æ¤œè¨Žã—ã¦ãã ã•ã„ã€‚")
    if data['top_fatigue_staff']:
        staff_list = ', '.join([str(s) for s in data['top_fatigue_staff'][:3]])
        recommendations.append(f"âš ï¸ ç–²åŠ´åº¦ãŒé«˜ã„è·å“¡ãŒã„ã¾ã™: {staff_list}")
    if not recommendations:
        recommendations.append("âœ… ç‰¹ã«ç·Šæ€¥ã®èª²é¡Œã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")

    content.extend([
        html.H4("ðŸ’¡ æŽ¨å¥¨äº‹é …"),
        html.Ul([html.Li(rec) for rec in recommendations]),
    ])

    return html.Div(content)


def page_reports(session: SessionData, metadata: Optional[dict]) -> html.Div:
    """
    ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¿ãƒ– - PowerPointãƒ¬ãƒãƒ¼ãƒˆã¨å„ç¨®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæ©Ÿèƒ½ã‚’æä¾›

    Args:
        session: SessionData ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        metadata: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¾žæ›¸

    Returns:
        ãƒ¬ãƒãƒ¼ãƒˆã‚¿ãƒ–ã®Dash HTMLã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
    """
    return html.Div([
        html.H3("ðŸ“„ ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"),
        html.P("åˆ†æžçµæžœã‚’å„ç¨®å½¢å¼ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã§ãã¾ã™ã€‚"),
        html.Hr(),

        # PowerPointãƒ¬ãƒãƒ¼ãƒˆ
        html.H4("ðŸ“Š PowerPointãƒ¬ãƒãƒ¼ãƒˆ"),
        html.P("å…¨åˆ†æžçµæžœã‚’PowerPointå½¢å¼ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ã¾ã™ã€‚", style={"color": "#666"}),
        html.Div([
            html.P("ðŸš§ ã“ã®æ©Ÿèƒ½ã¯ç¾åœ¨é–‹ç™ºä¸­ã§ã™ã€‚", style={"padding": "15px", "backgroundColor": "#fff3cd", "border": "1px solid #ffc107", "borderRadius": "5px"}),
            html.P("å®Ÿè£…äºˆå®šã®æ©Ÿèƒ½:", style={"marginTop": "15px", "fontWeight": "bold"}),
            html.Ul([
                html.Li("ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒžãƒªãƒ¼ã‚¹ãƒ©ã‚¤ãƒ‰"),
                html.Li("ä¸è¶³æ™‚é–“åˆ†æžã‚°ãƒ©ãƒ•"),
                html.Li("å…¬å¹³æ€§æŒ‡æ¨™ã®å¯è¦–åŒ–"),
                html.Li("ç–²åŠ´åº¦ãƒ»ä¼‘æš‡åˆ†æžãƒãƒ£ãƒ¼ãƒˆ"),
                html.Li("æŽ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚µãƒžãƒªãƒ¼")
            ])
        ], style={"padding": "20px", "border": "1px solid #ddd", "borderRadius": "5px", "marginBottom": "20px"}),

        # CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
        html.Hr(),
        html.H4("ðŸ“‘ CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"),
        html.P("ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’CSVå½¢å¼ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚", style={"color": "#666"}),
        html.Div([
            html.P("ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå¯èƒ½ãªãƒ‡ãƒ¼ã‚¿:", style={"fontWeight": "bold"}),
            html.Ul([
                html.Li("ä¸è¶³æ™‚é–“ãƒ‡ãƒ¼ã‚¿ (shortage_time.parquet)"),
                html.Li("å€‹äººåˆ¥å‹¤å‹™ãƒ‡ãƒ¼ã‚¿ (long_df)"),
                html.Li("ãƒ’ãƒ¼ãƒˆãƒžãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿ (heat_staff, heat_ratio)"),
                html.Li("å…¬å¹³æ€§æŒ‡æ¨™ (fairness_before, fairness_after)"),
                html.Li("ç–²åŠ´åº¦ã‚¹ã‚³ã‚¢ (fatigue_score)"),
                html.Li("ã‚³ã‚¹ãƒˆåˆ†æž (daily_cost, cost_benefit)")
            ]),
            html.P("ðŸš§ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½ã¯ä»Šå¾Œå®Ÿè£…äºˆå®šã§ã™ã€‚",
                   style={"padding": "10px", "backgroundColor": "#e7f3ff", "border": "1px solid #2196F3",
                          "borderRadius": "5px", "marginTop": "15px"})
        ], style={"padding": "20px", "border": "1px solid #ddd", "borderRadius": "5px"}),

        # åˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±
        html.Hr(),
        html.H4("ðŸ“¦ åˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ"),
        html.P("ç¾åœ¨ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ã¯ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã¦ã„ã¾ã™:", style={"marginBottom": "10px"}),
        _render_available_datasets(session, metadata)
    ])


def _render_available_datasets(session: SessionData, metadata: Optional[dict]) -> html.Div:
    """åˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒªã‚¹ãƒˆã‚’è¡¨ç¤º"""
    scenario_name = metadata.get("scenario") if metadata else None
    _, scenario = session.get_scenario_data(scenario_name)

    # åˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒã‚§ãƒƒã‚¯
    available_datasets = []
    dataset_checks = {
        "ä¸è¶³æ™‚é–“ãƒ‡ãƒ¼ã‚¿": "shortage_time",
        "å€‹äººåˆ¥ãƒ‡ãƒ¼ã‚¿": "long_df",
        "ãƒ’ãƒ¼ãƒˆãƒžãƒƒãƒ—": "heat_staff",
        "å…¬å¹³æ€§æŒ‡æ¨™": "fairness_before",
        "ç–²åŠ´åº¦ã‚¹ã‚³ã‚¢": "fatigue_score",
        "ä¼‘æš‡åˆ†æž": "leave_analysis",
        "ã‚³ã‚¹ãƒˆåˆ†æž": "daily_cost",
        "æœ€é©åŒ–ã‚¹ã‚³ã‚¢": "optimization_score_time",
        "éœ€è¦äºˆæ¸¬": "forecast",
        "æŽ¡ç”¨è¨ˆç”»": "hire_plan"
    }

    for label, attr_name in dataset_checks.items():
        if hasattr(scenario, attr_name):
            data = getattr(scenario, attr_name, None)
            if data is not None:
                if isinstance(data, pd.DataFrame) and not data.empty:
                    available_datasets.append(html.Li(f"âœ… {label}"))
                elif isinstance(data, dict) and len(data) > 0:
                    available_datasets.append(html.Li(f"âœ… {label}"))
                else:
                    available_datasets.append(html.Li(f"âš ï¸ {label} (ãƒ‡ãƒ¼ã‚¿ãªã—)", style={"color": "#999"}))
            else:
                available_datasets.append(html.Li(f"âŒ {label} (åˆ©ç”¨ä¸å¯)", style={"color": "#ccc"}))
        else:
            available_datasets.append(html.Li(f"âŒ {label} (åˆ©ç”¨ä¸å¯)", style={"color": "#ccc"}))

    return html.Ul(available_datasets, style={"columnCount": "2", "columnGap": "20px"})


# -----------------------------------------------------------------------------
# Heatmap page
# -----------------------------------------------------------------------------


def _create_heatmap_figure(scenario: ScenarioData, metadata: Optional[dict]) -> Any:
    """Create heatmap figure with responsive design and dynamic color scheme."""
    # Phase 2-2/2-3 å®Œå…¨å®Ÿè£…: ResponsiveVisualizationEngineã‚’å®Ÿéš›ã«ä½¿ç”¨
    device_type = metadata.get("device_type", "desktop") if metadata else "desktop"

    # Phase 3-5: ã‚«ãƒ©ãƒ¼ã‚¹ã‚­ãƒ¼ãƒ ã®å‹•çš„å–å¾—
    color_scheme_key = metadata.get("color_scheme", DEFAULT_COLOR_SCHEME) if metadata else DEFAULT_COLOR_SCHEME
    color_scheme = COLOR_SCHEMES.get(color_scheme_key, COLOR_SCHEMES[DEFAULT_COLOR_SCHEME])
    color_scale = color_scheme['scale']

    # Get visualization engine
    viz_engine = get_visualization_engine()

    if scenario.heat_staff.empty:
        return _empty_figure("ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

    # Phase 2-2: ãƒ¬ã‚¹ãƒãƒ³ã‚·ãƒ–ãƒ‡ã‚¶ã‚¤ãƒ³ã®å®Ÿè£…
    if viz_engine is not None and VISUALIZATION_ENGINE_AVAILABLE:
        # æ—§ã‚·ã‚¹ãƒ†ãƒ ã®ResponsiveVisualizationEngineã‚’ä½¿ç”¨
        fig = viz_engine.create_responsive_heatmap(
            data=scenario.heat_staff,
            title="å…¨ä½“ãƒ’ãƒ¼ãƒˆãƒžãƒƒãƒ—",
            device_type=device_type,
            progress_callback=None  # é€²æ—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¯ç¾çŠ¶ã§ã¯ä½¿ç”¨ã—ãªã„
        )
        # Phase 3-5: ã‚«ãƒ©ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«ã¨zç¯„å›²ã‚’æ‰‹å‹•ã§é©ç”¨ï¼ˆå‹•çš„ã‚«ãƒ©ãƒ¼ã‚¹ã‚­ãƒ¼ãƒ ï¼‰
        if hasattr(fig.data[0], 'colorscale'):
            fig.data[0].colorscale = color_scale
            fig.data[0].zmin = 0
            fig.data[0].zmax = scenario.heat_settings.zmax_default
    else:
        # Phase 3-5: ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆå‹•çš„ã‚«ãƒ©ãƒ¼ã‚¹ã‚­ãƒ¼ãƒ é©ç”¨ï¼‰
        fig = px.imshow(
            scenario.heat_staff,
            aspect="auto",
            color_continuous_scale=color_scale,
            zmin=0,
            zmax=scenario.heat_settings.zmax_default,
            labels=dict(x="æ—¥ä»˜", y="æ™‚é–“å¸¯", color="é…ç½®äººæ•°"),
            title="å…¨ä½“ãƒ’ãƒ¼ãƒˆãƒžãƒƒãƒ—"
        )
        fig.update_layout(
            height=500,
            margin=dict(l=20, r=20, t=60, b=20),
        )

    return fig


def _create_heatmap_stats(heat_staff: pd.DataFrame) -> html.Div:
    """Create summary statistics display for heatmap."""
    if heat_staff.empty:
        return html.Div()

    max_staff = float(heat_staff.max().max())
    min_staff = float(heat_staff.min().min())
    avg_staff = float(heat_staff.mean().mean())

    return html.Div([
        html.Div([
            html.Span("æœ€å¤§é…ç½®: ", style={"fontWeight": "bold"}),
            html.Span(f"{max_staff:.1f} äºº"),
        ], style={"marginRight": "20px", "display": "inline-block"}),
        html.Div([
            html.Span("æœ€å°é…ç½®: ", style={"fontWeight": "bold"}),
            html.Span(f"{min_staff:.1f} äºº"),
        ], style={"marginRight": "20px", "display": "inline-block"}),
        html.Div([
            html.Span("å¹³å‡é…ç½®: ", style={"fontWeight": "bold"}),
            html.Span(f"{avg_staff:.1f} äºº"),
        ], style={"display": "inline-block"}),
    ], style={"marginBottom": "20px", "padding": "15px", "backgroundColor": "#f8f9fa", "borderRadius": "5px"})


def page_heatmap(session: SessionData, metadata: Optional[dict]) -> html.Div:
    """Heatmap visualization tab showing staff allocation heat maps."""
    scenario_name = metadata.get("scenario") if metadata else None
    _, scenario = session.get_scenario_data(scenario_name)

    # Check if we have heatmap data
    if scenario.heat_staff.empty:
        return html.Div([
            html.H3("ãƒ’ãƒ¼ãƒˆãƒžãƒƒãƒ—"),
            html.P("ãƒ’ãƒ¼ãƒˆãƒžãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚heat_ALL.parquet ãƒ•ã‚¡ã‚¤ãƒ«ãŒå¿…è¦ã§ã™ã€‚"),
        ])

    # Create heatmap figure
    fig = _create_heatmap_figure(scenario, metadata)

    # Create summary statistics
    stats = _create_heatmap_stats(scenario.heat_staff)

    # Phase 2-3: é€²æ—è¡¨ç¤ºã®çµ±åˆ
    return html.Div([
        html.H3("ðŸ”¥ ãƒ’ãƒ¼ãƒˆãƒžãƒƒãƒ—"),
        html.P("æ™‚é–“å¸¯åˆ¥ãƒ»æ—¥ä»˜åˆ¥ã®é…ç½®äººæ•°ã‚’å¯è¦–åŒ–ã—ã¾ã™ã€‚"),
        html.Hr(),
        stats,
        # Phase 2-3: é€²æ—è¡¨ç¤ºã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã§ã‚°ãƒ©ãƒ•ã‚’ãƒ©ãƒƒãƒ—
        dcc.Loading(
            id="loading-heatmap",
            type="default",
            children=dcc.Graph(figure=fig, config={'displayModeBar': True}),
            color="#2196f3"
        ),
        html.Hr(),
        html.P(
            "ãƒ’ãƒ¼ãƒˆãƒžãƒƒãƒ—ã§ã¯ã€è‰²ã®æ¿ƒã•ãŒé…ç½®äººæ•°ã‚’è¡¨ã—ã¾ã™ã€‚é’ã„é ˜åŸŸã»ã©é…ç½®ãŒå¤šãã€è–„ã„é ˜åŸŸã¯é…ç½®ãŒå°‘ãªã„ã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚",
            style={"fontSize": "12px", "color": "#666"}
        ),
    ])


# -----------------------------------------------------------------------------
# Overview page
# -----------------------------------------------------------------------------


def _create_kpi_card(title: str, value: str) -> html.Div:
    """Create a KPI card component with accessibility support (Phase 3-3)."""
    return html.Div(
        [
            html.Div(
                title,
                id=f"kpi-title-{title.replace(' ', '-').lower()}",
                style={"fontSize": "14px", "color": "#666", "marginBottom": "8px"}
            ),
            html.Div(
                value,
                style={"fontSize": "28px", "fontWeight": "bold", "color": "#333"},
                **{"aria-label": f"{title}: {value}"}
            ),
        ],
        style={
            "backgroundColor": "white",
            "padding": "20px",
            "borderRadius": "8px",
            "boxShadow": "0 2px 8px rgba(0,0,0,0.1)",
            "textAlign": "center",
            "minWidth": "180px",
            "flex": "1",
        },
        role="region",
        **{"aria-label": f"KPIã‚«ãƒ¼ãƒ‰: {title}"}
    )


def _calculate_overview_kpis(scenario: ScenarioData) -> Dict[str, Any]:
    """Calculate key performance indicators for overview dashboard."""
    kpis = {}

    # Total shortage hours
    kpis['total_shortage_hours'] = _calculate_total_shortage_hours(scenario.shortage_time)

    # Total staff
    kpis['total_staff'] = len(scenario.roles) if scenario.roles else 0

    # Date range
    date_range = "N/A"
    if not scenario.heat_staff.empty and len(scenario.heat_staff.columns) > 0:
        date_range = f"{scenario.heat_staff.columns[0]} ~ {scenario.heat_staff.columns[-1]}"
    kpis['date_range'] = date_range

    # Average daily staff
    avg_daily_staff = 0.0
    long_df = scenario.get_dataset("long_df")
    if long_df is not None and not long_df.empty and 'ds' in long_df.columns and 'staff' in long_df.columns:
        try:
            avg_daily_staff = long_df.groupby(long_df['ds'].dt.date)['staff'].nunique().mean()
        except Exception:
            pass
    kpis['avg_daily_staff'] = avg_daily_staff

    return kpis


def _create_overview_charts(scenario: ScenarioData) -> List:
    """Create shortage charts for overview dashboard (role-based and employment-based)."""
    charts = []

    # Role-based shortage chart
    if not scenario.shortage_role_summary.empty and 'role' in scenario.shortage_role_summary.columns:
        role_df = scenario.shortage_role_summary
        if 'lack_h' in role_df.columns:
            top_roles = role_df.nlargest(10, 'lack_h') if len(role_df) > 10 else role_df
            role_chart = dcc.Graph(
                figure=px.bar(
                    top_roles,
                    x='role',
                    y='lack_h',
                    title="è·ç¨®åˆ¥ ä¸è¶³æ™‚é–“ TOP10",
                    labels={'role': 'è·ç¨®', 'lack_h': 'ä¸è¶³æ™‚é–“ (h)'},
                    color='lack_h',
                    color_continuous_scale='Reds',
                ),
                config={'displayModeBar': False}
            )
            charts.append(html.Div([role_chart], style={"flex": "1", "minWidth": "400px"}, role="img", **{'aria-label': 'è·ç¨®åˆ¥ä¸è¶³æ™‚é–“ã‚°ãƒ©ãƒ•'}))

    # Employment-based shortage chart
    if not scenario.shortage_employment_summary.empty and 'employment' in scenario.shortage_employment_summary.columns:
        emp_df = scenario.shortage_employment_summary
        if 'lack_h' in emp_df.columns:
            emp_chart = dcc.Graph(
                figure=px.bar(
                    emp_df,
                    x='employment',
                    y='lack_h',
                    title="å‹¤å‹™å½¢æ…‹åˆ¥ ä¸è¶³æ™‚é–“",
                    labels={'employment': 'å‹¤å‹™å½¢æ…‹', 'lack_h': 'ä¸è¶³æ™‚é–“ (h)'},
                    color='lack_h',
                    color_continuous_scale='Oranges',
                ),
                config={'displayModeBar': False}
            )
            charts.append(html.Div([emp_chart], style={"flex": "1", "minWidth": "400px"}, role="img", **{'aria-label': 'å‹¤å‹™å½¢æ…‹åˆ¥ä¸è¶³æ™‚é–“ã‚°ãƒ©ãƒ•'}))

    return charts


def page_overview(session: SessionData, metadata: Optional[dict]) -> html.Div:
    """Overview dashboard tab showing key metrics and summaries."""
    start_time = time.time()

    scenario_name = metadata.get("scenario") if metadata else None
    _, scenario = session.get_scenario_data(scenario_name)

    # Calculate KPIs
    kpis = _calculate_overview_kpis(scenario)

    # Create KPI cards
    kpi_cards = html.Div(
        [
            _create_kpi_card("ç·ä¸è¶³æ™‚é–“", f"{kpis['total_shortage_hours']:.1f} h"),
            _create_kpi_card("åˆ†æžæœŸé–“", kpis['date_range']),
            _create_kpi_card("ç·è·å“¡æ•°", f"{kpis['total_staff']} äºº"),
            _create_kpi_card("å¹³å‡é…ç½®", f"{kpis['avg_daily_staff']:.1f} äºº/æ—¥"),
        ],
        style={"display": "flex", "gap": "20px", "marginBottom": "30px", "flexWrap": "wrap"},
    )

    # Create charts
    charts = _create_overview_charts(scenario)

    # Charts container with accessibility
    charts_div = html.Div(
        charts,
        style={"display": "flex", "gap": "20px", "flexWrap": "wrap"},
        role="region",
        **{'aria-label': 'ã‚°ãƒ©ãƒ•ã‚»ã‚¯ã‚·ãƒ§ãƒ³'}
    ) if charts else html.P("ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚", role="status")

    # Log performance
    duration = time.time() - start_time
    log_performance("Overview Tab ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°", duration, {"scenario": scenario_name or "default"})

    # Return dashboard layout
    return html.Div(
        [
            html.H2("ðŸ“Š æ¦‚è¦ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰", **{'aria-level': '2'}),
            html.P("ã‚·ãƒ•ãƒˆã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®KPIã¨åŸºæœ¬çµ±è¨ˆã‚’è¡¨ç¤ºã—ã¾ã™ã€‚", role="doc-subtitle"),
            html.Hr(**{'aria-hidden': 'true'}),
            html.Div(kpi_cards, role="region", **{'aria-label': 'KPIã‚«ãƒ¼ãƒ‰ã‚»ã‚¯ã‚·ãƒ§ãƒ³'}),
            html.Hr(**{'aria-hidden': 'true'}),
            dcc.Loading(
                id="loading-overview-charts",
                type="default",
                children=charts_div,
                color="#2196f3"
            ),
        ],
        role="article",
        **{'aria-label': 'æ¦‚è¦ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰'}
    )


# -----------------------------------------------------------------------------
# Shortage page
# -----------------------------------------------------------------------------


def page_shortage(session: SessionData, metadata: Optional[dict]) -> html.Div:
    scenario_name = metadata.get("scenario") if metadata else None
    _, scenario = session.get_scenario_data(scenario_name)

    if scenario.shortage_time.empty and scenario.shortage_role_summary.empty:
        return html.Div(
            [
                html.H3("ä¸è¶³ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“"),
                html.P("ä¸è¶³é–¢é€£ã®ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"),
            ]
        )

    total_lack_hours = _calculate_total_shortage_hours(scenario.shortage_time)
    cards = []
    if total_lack_hours > 0:
        cards.append(html.Div([html.H4("ç·ä¸è¶³æ™‚é–“"), html.P(f"{total_lack_hours:.1f} h")]))

    if not scenario.shortage_role_summary.empty:
        cards.append(
            html.Div(
                [
                    html.H4("è·ç¨®åˆ¥ ä¸è¶³æ™‚é–“"),
                    dcc.Graph(
                        figure=px.bar(
                            scenario.shortage_role_summary,
                            x="role",
                            y="lack_h",
                            labels={"role": "Role", "lack_h": "ä¸è¶³æ™‚é–“"},
                        )
                    ),
                ]
            )
        )

    if not scenario.shortage_employment_summary.empty:
        cards.append(
            html.Div(
                [
                    html.H4("å‹¤å‹™å½¢æ…‹åˆ¥ ä¸è¶³æ™‚é–“"),
                    dcc.Graph(
                        figure=px.bar(
                            scenario.shortage_employment_summary,
                            x="employment",
                            y="lack_h",
                            labels={"employment": "Employment", "lack_h": "ä¸è¶³æ™‚é–“"},
                        )
                    ),
                ]
            )
        )

    return html.Div(cards or [html.P("ä¸è¶³ã«é–¢ã™ã‚‹é›†è¨ˆãŒã‚ã‚Šã¾ã›ã‚“ã€‚")])


# -----------------------------------------------------------------------------
# Misc helpers for tests
# -----------------------------------------------------------------------------


def update_metadata_on_scenario(
    scenario_value: Optional[str], session_id: Optional[str], metadata: Optional[dict]
) -> Dict[str, object]:
    session = get_session(session_id)
    if session is None:
        return {}
    return session.metadata(scenario_value)


__all__ = [
    "HeatmapSettings",
    "ScenarioData",
    "SCENARIO_ARTIFACT_EXPECTATIONS",
    "SessionData",
    "_build_comparison_heatmap_figure",
    "_calculate_total_shortage_hours",
    "_collect_missing_artifacts",
    "_ensure_artifacts_from_root",
    "load_session_data_from_zip",
    "page_logic",
    "page_mind_reader",
    "page_gap_analysis",
    "page_individual",
    "page_team",
    "page_fatigue",
    "page_leave",
    "page_fairness",
    "page_optimization",
    "page_forecast",
    "page_hire_plan",
    "page_cost",
    "page_summary",
    "page_reports",
    "page_heatmap",
    "page_overview",
    "page_shortage",
    "page_blueprint",
    "register_session",
    "get_session",
    "get_dataset",
    "update_heatmap",
    "update_heatmap_comparison_panel",
    # Phase 1: Memory Management Integration
    "initialize_memory_manager",
    "get_memory_manager",
    "cleanup_expired_sessions",
    "start_session_cleanup",
    # Phase 2-2/2-3: Visualization Engine Integration
    "initialize_visualization_engine",
    "get_visualization_engine",
    "create_progress_indicator",
]
